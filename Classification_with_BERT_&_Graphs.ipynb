{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_with_BERT_&_Graphs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOUKDfp3gduAj6qx2yhHL/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdrienHdz/Deep_Learning_HEC/blob/master/Classification_with_BERT_%26_Graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaHbPsJ3dBlo",
        "colab_type": "text"
      },
      "source": [
        "# Introduction:\n",
        "In this homework I used several deep learning models in order to perform classification with 5 labels of scientific research papers. \n",
        "* Firstly, I will show my Bert for sequence classification model with Pytorch and the huggingface library. \n",
        "* Then, I will go through different graph deep learning models with the package Stellargraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnrgCaL99k2u",
        "colab_type": "text"
      },
      "source": [
        "# We load the **librairies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVwUojUzh4Is",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "d79c11ef-9168-44d5-cbfc-34c8fc26c579"
      },
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import networkx as nx\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbBKU87ch69Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f167b496-fc27-493d-81cd-91f5a3820353"
      },
      "source": [
        "import torch\n",
        "\n",
        "# We tell pytorch we want to use Google Colab's GPU\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCov_6Jyi6vH",
        "colab_type": "text"
      },
      "source": [
        "# We load the data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCFAVMpw_Otd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ae78c2d-ff61-4657-b45e-3d4e91a1be35"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "data_dic = {};\n",
        "for dirname, _, filenames in os.walk('/content/drive/My Drive/Colab Notebooks'):\n",
        "    for filename in filenames:\n",
        "        if \".csv\" in filename:\n",
        "          data_dic[filename.split('.')[0]] = pd.read_csv(os.path.join(dirname, filename),index_col=\"id\");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkCwUbqdjvNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be571f2b-9190-40ea-b8e2-3907430a3bf9"
      },
      "source": [
        "data_dic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'reference':         id.1\n",
              " id          \n",
              " 0      22305\n",
              " 0      22491\n",
              " 1       9243\n",
              " 1      10943\n",
              " 1      14322\n",
              " ...      ...\n",
              " 25557  22946\n",
              " 25558   8691\n",
              " 25558  21420\n",
              " 25560   3168\n",
              " 25560   6353\n",
              " \n",
              " [73313 rows x 1 columns], 'sample':        label\n",
              " id          \n",
              " 1          2\n",
              " 2          0\n",
              " 4          3\n",
              " 5          3\n",
              " 7          4\n",
              " ...      ...\n",
              " 25553      4\n",
              " 25556      1\n",
              " 25558      4\n",
              " 25559      3\n",
              " 25560      2\n",
              " \n",
              " [12782 rows x 1 columns], 'test': Empty DataFrame\n",
              " Columns: []\n",
              " Index: [1, 2, 4, 5, 7, 10, 12, 14, 15, 16, 17, 19, 21, 22, 23, 25, 26, 27, 28, 29, 34, 35, 37, 39, 42, 45, 47, 49, 50, 51, 52, 57, 60, 61, 63, 64, 65, 66, 67, 70, 72, 73, 74, 76, 80, 83, 84, 85, 88, 89, 96, 100, 102, 104, 105, 109, 111, 117, 118, 121, 124, 125, 126, 127, 128, 129, 130, 131, 135, 136, 138, 139, 143, 148, 149, 150, 151, 154, 157, 158, 160, 161, 163, 164, 165, 166, 168, 171, 175, 176, 179, 181, 182, 183, 184, 186, 187, 191, 192, 193, ...]\n",
              " \n",
              " [12782 rows x 0 columns], 'text':                                                    title\n",
              " id                                                      \n",
              " 0      interactive visual exploration of neighbor bas...\n",
              " 1      autodomainmine a graphical data mining system ...\n",
              " 2      anipqo almost non intrusive parametric query o...\n",
              " 3      relational division four algorithms and their ...\n",
              " 4      selection and ranking of text from highly impe...\n",
              " ...                                                  ...\n",
              " 25556      dynamic typing in a statically typed language\n",
              " 25557  maintaining materialized views in distributed ...\n",
              " 25558     learning sparse metrics via linear programming\n",
              " 25559             computer assisted reasoning with mizar\n",
              " 25560  characterization of a large web site populatio...\n",
              " \n",
              " [25561 rows x 1 columns], 'train':        label\n",
              " id          \n",
              " 0          1\n",
              " 3          1\n",
              " 6          1\n",
              " 8          0\n",
              " 9          0\n",
              " ...      ...\n",
              " 25547      4\n",
              " 25548      3\n",
              " 25554      2\n",
              " 25555      2\n",
              " 25557      1\n",
              " \n",
              " [12779 rows x 1 columns]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S3yzD17pgps",
        "colab_type": "text"
      },
      "source": [
        "# BERT for Sequence Classification\n",
        "My code has been adapted from Chris McCormick and Nick Ryan BERT Fine-Tuning tutorial:\n",
        "https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpAKUuZAh7nJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "979871de-64f1-4544-8c7c-e1e373daccf5"
      },
      "source": [
        "# We merge the data from train and text into a new DataFrame called df with 3 columns: id, label and title.\n",
        "train = pd.DataFrame(data_dic[\"train\"])\n",
        "text = pd.DataFrame(data_dic[\"text\"])\n",
        "df = pd.merge(train, text, on=[\"id\"])\n",
        "\n",
        "print('Number of observations: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of observations: 12,779\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>interactive visual exploration of neighbor bas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>relational division four algorithms and their ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>simplifying xml schema effortless handling of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>funbase a function based information managemen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>inverted matrix efficient discovery of frequen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    label                                              title\n",
              "id                                                          \n",
              "0       1  interactive visual exploration of neighbor bas...\n",
              "3       1  relational division four algorithms and their ...\n",
              "6       1  simplifying xml schema effortless handling of ...\n",
              "8       0  funbase a function based information managemen...\n",
              "9       0  inverted matrix efficient discovery of frequen..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5ztHTNh7zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Then we extract the research papers labels and titles as numpy arrays.\n",
        "titles = df.title.values\n",
        "labels = df.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-T_WpWenW1N",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RhnutYLh7ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For Bert, we have to tokenize our text using the function BertTokenizer from the package transformers\n",
        "# Here, we use the best-base-uncased pretrained model, a list of bert models for the package transformers can be found here https://huggingface.co/transformers/pretrained_models.html\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CotiO6jh7k6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e13e27c7-5eb5-4ee0-93f2-6778e06aa9c1"
      },
      "source": [
        "# Let's visualize the tokenization\n",
        "print('Original sentence: ', titles[0])\n",
        "print('Tokenized sentence: ', tokenizer.tokenize(titles[0]))\n",
        "print('Tokens IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(titles[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence:  interactive visual exploration of neighbor based patterns in data streams\n",
            "Tokenized sentence:  ['interactive', 'visual', 'exploration', 'of', 'neighbor', 'based', 'patterns', 'in', 'data', 'streams']\n",
            "Tokens IDs:  [9123, 5107, 8993, 1997, 11429, 2241, 7060, 1999, 2951, 9199]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVUDo_Mxh7i2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e5456fa7-046f-405c-f180-2720a97f6fa5"
      },
      "source": [
        "# We apply the tokenization for all the sentences\n",
        "# We will save all the tokens IDs into a dictionnary called inputs_id\n",
        "inputs_id = []\n",
        "\n",
        "# The function tokenizer.encode, allows to tokenize a sentence, to use CLS token to the start of a sentence and SEP to the end of a sentence, \n",
        "# and to map each token to their ids. We use a loop to go through all the sentences of our dataset df.\n",
        "for t in titles:\n",
        "  encoded_sent = tokenizer.encode(t,\n",
        "                                  add_special_tokens=True)\n",
        "  \n",
        "  inputs_id.append(encoded_sent)\n",
        "\n",
        "print('Original sentence: ', titles[0])\n",
        "print(' Tokens IDs:', inputs_id[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence:  interactive visual exploration of neighbor based patterns in data streams\n",
            " Tokens IDs: [101, 9123, 5107, 8993, 1997, 11429, 2241, 7060, 1999, 2951, 9199, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgIJvDa_sIbs",
        "colab_type": "text"
      },
      "source": [
        "# Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsPLmstNh7eu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5df390f1-b5a7-491c-9063-b5a38d985b2f"
      },
      "source": [
        "# Let's check the length of our longest sentence\n",
        "print('Max sentence length: ', max([len(sen) for sen in inputs_id]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3-kHAazh7cc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b4689c4-1ae5-4aeb-ff76-30e24b7eb7e7"
      },
      "source": [
        "# We use the pad sequences function from Keras to pad our sequences to the same size.\n",
        "# We will use the max length of 42, which is the maximum length of our longest sentence.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 42\n",
        "# We pad with the value 0 added to the end of the sentene up to 42 ids in the same sentence.\n",
        "inputs_id = pad_sequences(inputs_id, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b99LWS7ix1mC",
        "colab_type": "text"
      },
      "source": [
        "# Attention Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjonnihah7Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As we padded sequences, we introduced a lot of ids = 0, that are not real words.\n",
        "# We use an attention mask in order to differentiate real ids from padding (ids = 0)\n",
        "\n",
        "attention_masks = []\n",
        "\n",
        "for sentence in inputs_id:\n",
        "  att_mask = [int(token_id > 0) for token_id in sentence]\n",
        "  attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzb3f-vxy7iG",
        "colab_type": "text"
      },
      "source": [
        "# Split data into training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCVr91YCzBxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(inputs_id, labels,\n",
        "                                                                                   random_state=2020, test_size=0.2)\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2020, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMI1HNDt0Cdj",
        "colab_type": "text"
      },
      "source": [
        "# We convert our numpy arrays to PyTorch data in order to input them in the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2xid-krzCCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgdRPsihzCMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use Pytorch DataLoader during the training phase because it is more efficient than a for loop as all the data won't be loaded into memory.\n",
        "# Therefor, we need to specify our batch_size that will be used during the training phase and it will be 32.\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3i3pLHZtJ7A",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1B_rs-stMM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "307f958a-9e2c-436e-cacc-1e61b1f93d44"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# we load the pretrained bert base uncased model, and we specify that we have a multi classification task with 5 different labels.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 5, \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n9ifHv1vx_k",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq50S5_5u0qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use the Adam with weight decay fix from the huggingface library.\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8) # args.adam_epsilon  - default is 1e-8.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLZqby9QzCX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQmjjTKqzzgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWQVjwb3zzk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2hFR35Mzzd8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c76b211c-fc50-4de3-9a39-7e13e403597d"
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Clear any previously calculated gradients before performing a backward pass.\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # We fix the norm of the gradients to 1.0 to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After each training epoch we calculate the validation accuracy.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. \n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "  Batch    40  of    320.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    320.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    320.    Elapsed: 0:00:21.\n",
            "  Batch   160  of    320.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    320.    Elapsed: 0:00:35.\n",
            "  Batch   240  of    320.    Elapsed: 0:00:42.\n",
            "  Batch   280  of    320.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 1.17\n",
            "  Training epcoh took: 0:00:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:00:03\n",
            "======== Epoch 2 / 4 ========\n",
            "  Batch    40  of    320.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    320.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    320.    Elapsed: 0:00:21.\n",
            "  Batch   160  of    320.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    320.    Elapsed: 0:00:35.\n",
            "  Batch   240  of    320.    Elapsed: 0:00:42.\n",
            "  Batch   280  of    320.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.80\n",
            "  Training epcoh took: 0:00:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.71\n",
            "  Validation took: 0:00:03\n",
            "======== Epoch 3 / 4 ========\n",
            "  Batch    40  of    320.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    320.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    320.    Elapsed: 0:00:21.\n",
            "  Batch   160  of    320.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    320.    Elapsed: 0:00:35.\n",
            "  Batch   240  of    320.    Elapsed: 0:00:42.\n",
            "  Batch   280  of    320.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:00:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:03\n",
            "======== Epoch 4 / 4 ========\n",
            "  Batch    40  of    320.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    320.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    320.    Elapsed: 0:00:21.\n",
            "  Batch   160  of    320.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    320.    Elapsed: 0:00:35.\n",
            "  Batch   240  of    320.    Elapsed: 0:00:42.\n",
            "  Batch   280  of    320.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:00:56\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJO-2gU6N2ty",
        "colab_type": "text"
      },
      "source": [
        "### We get a validation accuracy of **73% !**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whBd5ZOT-ZxA",
        "colab_type": "text"
      },
      "source": [
        "# Predict on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n4uX0wMzzb5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e9d695df-ed6a-41e2-acfb-040ffc838537"
      },
      "source": [
        "# Load the test dataset\n",
        "test_final = pd.DataFrame(data_dic[\"test\"])\n",
        "# We merge the test_final and text dataset on id\n",
        "test_final = pd.merge(test_final, text,on=[\"id\"])\n",
        "df = test_final\n",
        "\n",
        "# We check the number of sentences\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create titles list\n",
        "titles_test = df.title.values\n",
        "#labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence\n",
        "for t in titles_test:\n",
        "\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        t,                   \n",
        "                        add_special_tokens = True\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "#prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 12,782\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxA5FAnkDOb1",
        "colab_type": "text"
      },
      "source": [
        "# We generate the labels prediction for the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzsnFGWPDJG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3a63d6f0-39ad-455b-9bf0-9001903632bc"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions = []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  b_input_ids, b_input_mask = batch\n",
        "\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  \n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 12,782 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0LLje7DE2nD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We get the predictions for each sentence \n",
        "predictions_test = []\n",
        "for i in range(len(predictions)):\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  predictions_test.append(pred_labels_i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJXwU68oFKGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We put the predictions into the format one column\n",
        "flat_list = []\n",
        "for sublist in predictions_test:\n",
        "    for item in sublist:\n",
        "        flat_list.append(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0oj4m5oFZEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We save the predicted labels in csv format\n",
        "np.savetxt(\"bert.csv\", flat_list, delimiter=\",\", fmt='%s')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK3LWTthFqqA",
        "colab_type": "text"
      },
      "source": [
        "# Graph Neural Networks with Stellargraph\n",
        "\n",
        "This code is based on Stellargraph official tutorial:\n",
        "https://github.com/stellargraph/stellargraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQMAGxzHSRoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ca02f50-365f-48f4-c52c-568f236309d3"
      },
      "source": [
        "!pip install stellargraph\n",
        "import stellargraph as sg\n",
        "from stellargraph.data import UnsupervisedSampler\n",
        "from stellargraph.mapper import Attri2VecLinkGenerator, Attri2VecNodeGenerator\n",
        "from stellargraph.layer import Attri2Vec, link_classification\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import stellargraph as sg\n",
        "from stellargraph.layer import GraphSAGE\n",
        "from stellargraph.mapper import GraphSAGENodeGenerator\n",
        "\n",
        "from stellargraph.layer import DirectedGraphSAGE\n",
        "\n",
        "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
        "from sklearn import preprocessing, feature_extraction, model_selection\n",
        "from stellargraph import datasets\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stellargraph in /usr/local/lib/python3.6/dist-packages (0.11.1)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from stellargraph) (3.6.0)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from stellargraph) (3.2.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.6/dist-packages (from stellargraph) (2.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from stellargraph) (1.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from stellargraph) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from stellargraph) (2.2.0rc2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from stellargraph) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from stellargraph) (1.18.2)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->stellargraph) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->stellargraph) (1.10.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->stellargraph) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->stellargraph) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->stellargraph) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->stellargraph) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.2->stellargraph) (4.4.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->stellargraph) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->stellargraph) (0.14.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.10.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.2.0rc0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.28.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.2.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (1.12.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (2.21.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (1.18.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow>=2.1.0->stellargraph) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (1.7.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (1.6.0.post3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (0.4.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (1.15.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (0.9.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (2020.4.5.1)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (1.0.3)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (1.3.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (0.15.2)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (1.16.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1.0->stellargraph) (3.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim>=3.4.0->stellargraph) (1.51.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zfvdudSEjgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "3691e5fd-72b4-44d6-c05f-6b12cb0081fd"
      },
      "source": [
        "# We first create the edge list data_frame from our reference dataset.\n",
        "data_dic[\"reference\"] = data_dic[\"reference\"].rename_axis(\"id.0\").reset_index()\n",
        "edgelist = data_dic[\"reference\"].rename(columns={'id.0':'target', 'id.1':'source'})\n",
        "edgelist.head() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>22305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>22491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>9243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>10943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>14322</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target  source\n",
              "0       0   22305\n",
              "1       0   22491\n",
              "2       1    9243\n",
              "3       1   10943\n",
              "4       1   14322"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyPdf-T7SZOW",
        "colab_type": "text"
      },
      "source": [
        "# Stemmer and removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-feGBNzFNiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will remove englishs stopwords and will use a stemmer in order to try remove some \"noise\" in the data\n",
        "# and to reduce the size of our matrix\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78mhYriAFdpm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "7fafef68-4b28-446c-a533-ba17c74fe3d2"
      },
      "source": [
        "# We add to our dictionnary our graph that combines both train and test data.\n",
        "data_dic[\"graph\"] = data_dic[\"train\"].append(data_dic[\"test\"], sort=True).drop(labels = \"label\", axis=1)\n",
        "data_dic[\"graph\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25553</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25556</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25558</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25559</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25560</th>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25561 rows  0 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: [0, 3, 6, 8, 9, 11, 13, 18, 20, 24, 30, 31, 32, 33, 36, 38, 40, 41, 43, 44, 46, 48, 53, 54, 55, 56, 58, 59, 62, 68, 69, 71, 75, 77, 78, 79, 81, 82, 86, 87, 90, 91, 92, 93, 94, 95, 97, 98, 99, 101, 103, 106, 107, 108, 110, 112, 113, 114, 115, 116, 119, 120, 122, 123, 132, 133, 134, 137, 140, 141, 142, 144, 145, 146, 147, 152, 153, 155, 156, 159, 162, 167, 169, 170, 172, 173, 174, 177, 178, 180, 185, 188, 189, 190, 194, 195, 196, 197, 198, 199, ...]\n",
              "\n",
              "[25561 rows x 0 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4T6SRgRJSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We add to our graph DataFrame the correspind title from the text dataset to each id of the graph dataset\n",
        "test = pd.merge(data_dic[\"graph\"], data_dic[\"text\"], on=\"id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pce5r1FWZMTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create a column with no stopwords and no stemmer\n",
        "test[\"title_clean_stopwords\"] = test[\"title\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "test[\"title_clean_stopwords_stemmer\"] = test[\"title_clean_stopwords\"].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split() ]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw2PCTg0WKc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "5d6e45e6-4e10-42fa-b183-09827fe170fa"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>title_clean_stopwords</th>\n",
              "      <th>title_clean_stopwords_stemmer</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>interactive visual exploration of neighbor bas...</td>\n",
              "      <td>interactive visual exploration neighbor based ...</td>\n",
              "      <td>interact visual explor neighbor base pattern d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>relational division four algorithms and their ...</td>\n",
              "      <td>relational division four algorithms performance</td>\n",
              "      <td>relat divis four algorithm perform</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>simplifying xml schema effortless handling of ...</td>\n",
              "      <td>simplifying xml schema effortless handling non...</td>\n",
              "      <td>simplifi xml schema effortless handl nondeterm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>funbase a function based information managemen...</td>\n",
              "      <td>funbase function based information management ...</td>\n",
              "      <td>funbas function base inform manag system</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>inverted matrix efficient discovery of frequen...</td>\n",
              "      <td>inverted matrix efficient discovery frequent i...</td>\n",
              "      <td>invert matrix effici discoveri frequent item l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                title  ...                      title_clean_stopwords_stemmer\n",
              "id                                                     ...                                                   \n",
              "0   interactive visual exploration of neighbor bas...  ...  interact visual explor neighbor base pattern d...\n",
              "3   relational division four algorithms and their ...  ...                 relat divis four algorithm perform\n",
              "6   simplifying xml schema effortless handling of ...  ...  simplifi xml schema effortless handl nondeterm...\n",
              "8   funbase a function based information managemen...  ...           funbas function base inform manag system\n",
              "9   inverted matrix efficient discovery of frequen...  ...  invert matrix effici discoveri frequent item l...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYOENzLKZTcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use the scikit-learn function CountVectorizer to convert the titles to a matrix of token counts\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "test[\"sentence_vectors\"]  = vectorizer.fit_transform(test[\"title_clean_stopwords_stemmer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J7xhxwOWRi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "4da12cf0-7326-4956-c64d-2bf760e0ed30"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>title_clean_stopwords</th>\n",
              "      <th>title_clean_stopwords_stemmer</th>\n",
              "      <th>sentence_vectors</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>interactive visual exploration of neighbor bas...</td>\n",
              "      <td>interactive visual exploration neighbor based ...</td>\n",
              "      <td>interact visual explor neighbor base pattern d...</td>\n",
              "      <td>(0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>relational division four algorithms and their ...</td>\n",
              "      <td>relational division four algorithms performance</td>\n",
              "      <td>relat divis four algorithm perform</td>\n",
              "      <td>(0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>simplifying xml schema effortless handling of ...</td>\n",
              "      <td>simplifying xml schema effortless handling non...</td>\n",
              "      <td>simplifi xml schema effortless handl nondeterm...</td>\n",
              "      <td>(0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>funbase a function based information managemen...</td>\n",
              "      <td>funbase function based information management ...</td>\n",
              "      <td>funbas function base inform manag system</td>\n",
              "      <td>(0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>inverted matrix efficient discovery of frequen...</td>\n",
              "      <td>inverted matrix efficient discovery frequent i...</td>\n",
              "      <td>invert matrix effici discoveri frequent item l...</td>\n",
              "      <td>(0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                title  ...                                   sentence_vectors\n",
              "id                                                     ...                                                   \n",
              "0   interactive visual exploration of neighbor bas...  ...    (0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...\n",
              "3   relational division four algorithms and their ...  ...    (0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...\n",
              "6   simplifying xml schema effortless handling of ...  ...    (0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...\n",
              "8   funbase a function based information managemen...  ...    (0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...\n",
              "9   inverted matrix efficient discovery of frequen...  ...    (0, 3926)\\t1\\n  (0, 8731)\\t1\\n  (0, 2762)\\t1...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm82Wl1_XLa4",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWJHQ4ptZVgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We tokenize each sentence with the keras function Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer_inputs = Tokenizer()\n",
        "tokenizer_inputs.fit_on_texts(test[\"title_clean_stopwords_stemmer\"])\n",
        "test[\"sequence\"] = tokenizer_inputs.texts_to_sequences(test[\"title_clean_stopwords_stemmer\"])\n",
        "feature_names = [\"w_{}\".format(ii) for ii in range(9155)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6aQXSljZYJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create the node data DataFrame according to Stellargraph tutorial\n",
        "# For each title id, we count the number of times that a word is present in the title and we create a matrix.\n",
        "node_data = pd.DataFrame(tokenizer_inputs.texts_to_matrix(test[\"title_clean_stopwords_stemmer\"], mode='count'),\n",
        "                                 dtype=int,columns=feature_names,\n",
        "                                 index=test.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q12DHcdZbn_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4b48c5ab-a927-41c5-b2d2-d1d4db9e37a9"
      },
      "source": [
        "node_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>w_0</th>\n",
              "      <th>w_1</th>\n",
              "      <th>w_2</th>\n",
              "      <th>w_3</th>\n",
              "      <th>w_4</th>\n",
              "      <th>w_5</th>\n",
              "      <th>w_6</th>\n",
              "      <th>w_7</th>\n",
              "      <th>w_8</th>\n",
              "      <th>w_9</th>\n",
              "      <th>w_10</th>\n",
              "      <th>w_11</th>\n",
              "      <th>w_12</th>\n",
              "      <th>w_13</th>\n",
              "      <th>w_14</th>\n",
              "      <th>w_15</th>\n",
              "      <th>w_16</th>\n",
              "      <th>w_17</th>\n",
              "      <th>w_18</th>\n",
              "      <th>w_19</th>\n",
              "      <th>w_20</th>\n",
              "      <th>w_21</th>\n",
              "      <th>w_22</th>\n",
              "      <th>w_23</th>\n",
              "      <th>w_24</th>\n",
              "      <th>w_25</th>\n",
              "      <th>w_26</th>\n",
              "      <th>w_27</th>\n",
              "      <th>w_28</th>\n",
              "      <th>w_29</th>\n",
              "      <th>w_30</th>\n",
              "      <th>w_31</th>\n",
              "      <th>w_32</th>\n",
              "      <th>w_33</th>\n",
              "      <th>w_34</th>\n",
              "      <th>w_35</th>\n",
              "      <th>w_36</th>\n",
              "      <th>w_37</th>\n",
              "      <th>w_38</th>\n",
              "      <th>w_39</th>\n",
              "      <th>...</th>\n",
              "      <th>w_9115</th>\n",
              "      <th>w_9116</th>\n",
              "      <th>w_9117</th>\n",
              "      <th>w_9118</th>\n",
              "      <th>w_9119</th>\n",
              "      <th>w_9120</th>\n",
              "      <th>w_9121</th>\n",
              "      <th>w_9122</th>\n",
              "      <th>w_9123</th>\n",
              "      <th>w_9124</th>\n",
              "      <th>w_9125</th>\n",
              "      <th>w_9126</th>\n",
              "      <th>w_9127</th>\n",
              "      <th>w_9128</th>\n",
              "      <th>w_9129</th>\n",
              "      <th>w_9130</th>\n",
              "      <th>w_9131</th>\n",
              "      <th>w_9132</th>\n",
              "      <th>w_9133</th>\n",
              "      <th>w_9134</th>\n",
              "      <th>w_9135</th>\n",
              "      <th>w_9136</th>\n",
              "      <th>w_9137</th>\n",
              "      <th>w_9138</th>\n",
              "      <th>w_9139</th>\n",
              "      <th>w_9140</th>\n",
              "      <th>w_9141</th>\n",
              "      <th>w_9142</th>\n",
              "      <th>w_9143</th>\n",
              "      <th>w_9144</th>\n",
              "      <th>w_9145</th>\n",
              "      <th>w_9146</th>\n",
              "      <th>w_9147</th>\n",
              "      <th>w_9148</th>\n",
              "      <th>w_9149</th>\n",
              "      <th>w_9150</th>\n",
              "      <th>w_9151</th>\n",
              "      <th>w_9152</th>\n",
              "      <th>w_9153</th>\n",
              "      <th>w_9154</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  9155 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    w_0  w_1  w_2  w_3  w_4  ...  w_9150  w_9151  w_9152  w_9153  w_9154\n",
              "id                           ...                                        \n",
              "0     0    1    0    1    0  ...       0       0       0       0       0\n",
              "3     0    0    0    0    0  ...       0       0       0       0       0\n",
              "6     0    0    0    0    0  ...       0       0       0       0       0\n",
              "8     0    1    1    0    0  ...       0       0       0       0       0\n",
              "9     0    0    0    0    0  ...       0       0       0       0       0\n",
              "\n",
              "[5 rows x 9155 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z43TIQEgZgAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b587d16-db4a-4a85-f949-8a2b4379a4eb"
      },
      "source": [
        "# for each tokenizer_inputs we store its word index into a new DataFrame called word2index_inputs\n",
        "word2index_inputs = tokenizer_inputs.word_index\n",
        "\n",
        "# We determine the maximul length of our dataset\n",
        "max_len_input = max(len(s) for s in test[\"sequence\"])\n",
        "print(f'The longest title contains {max_len_input} tokens')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The longest title contains 22 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1iyddJtZjB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "a25883a1-6f39-4460-c91a-9066fc4eed5b"
      },
      "source": [
        "node_subjects = data_dic[\"train\"][\"label\"]\n",
        "set(node_subjects)\n",
        "node_subjects"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id\n",
              "0        1\n",
              "3        1\n",
              "6        1\n",
              "8        0\n",
              "9        0\n",
              "        ..\n",
              "25547    4\n",
              "25548    3\n",
              "25554    2\n",
              "25555    2\n",
              "25557    1\n",
              "Name: label, Length: 12779, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NrMlS70aMu_",
        "colab_type": "text"
      },
      "source": [
        "# Get the data ready to input our graph model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcanDTVjZwh4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "fcdb9977-a316-44ed-f926-661cf6a167b8"
      },
      "source": [
        "G = sg.StellarGraph(nodes={\"paper\": node_data},\n",
        "                    edges={\"citation\" : edgelist})\n",
        "print(G.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "StellarGraph: Undirected multigraph\n",
            " Nodes: 25561, Edges: 73313\n",
            "\n",
            " Node types:\n",
            "  paper: [25561]\n",
            "    Features: float32 vector, length 9155\n",
            "    Edge types: paper-citation->paper\n",
            "\n",
            " Edge types:\n",
            "    paper-citation->paper: [73313]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRj4miFVnrms",
        "colab_type": "text"
      },
      "source": [
        "# We split and transform the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGbgAValZ2gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We convert to numeric arrays. \n",
        "# The target variable is transformed into hot-one vectors.\n",
        "train_subjects, test_subjects = model_selection.train_test_split(node_subjects, test_size=0.2, stratify=node_subjects)\n",
        "target_encoding = preprocessing.LabelBinarizer()\n",
        "\n",
        "train_targets = target_encoding.fit_transform(train_subjects)\n",
        "test_targets = target_encoding.transform(test_subjects)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddaF-5BKZ4H4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We choose a batch size of 50 for out training and we will use 2 layers. The first one will have 10 nodes and the second one 5 nodes.\n",
        "batch_size = 50\n",
        "num_samples = [10, 5]\n",
        "# The graph generator \"allows to send the node features in samples subgraphs to keras\"\n",
        "generator = GraphSAGENodeGenerator(G=G, batch_size=batch_size, num_samples=num_samples, seed=2020)\n",
        "# The generator.flow function allows to \"create iterators over nodes that should be used for train, validate and evaluate the model.\"\n",
        "train_gen = generator.flow(train_subjects.index, train_targets, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB6vMkKcZ6B3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cdca46ec-7cc6-4135-b155-2348abf5b578"
      },
      "source": [
        "# We specify the hyperparameters.\n",
        "# We use 2 layers with 32 dimensional features for each layer.\n",
        "# We use a dropout of 0.05\n",
        "graphsage_model = GraphSAGE(\n",
        "    layer_sizes=[32, 32], \n",
        "    generator=generator, \n",
        "    bias=True, \n",
        "    dropout=0.05 )\n",
        "x_inp, x_out = graphsage_model.build()\n",
        "# The model can predict 5 categories of scientific research papers\n",
        "prediction = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: The 'build' method is deprecated, use 'in_out_tensors' instead.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fwLo2FLZ8dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The input of our keras model is the under the shape of a graph network\n",
        "# The output of our keras model is predictions from a softmax layer\n",
        "model = Model(inputs=x_inp, outputs=prediction)\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(lr=0.001),\n",
        "    loss=losses.categorical_crossentropy,\n",
        "    metrics=[\"acc\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6dL6JxrZ-Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The test gen is our validation set that allows to tune the hyperparameters\n",
        "test_gen = generator.flow(test_subjects.index, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7wnT23KaAxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "0451bed3-6082-4460-ae39-af3abb7f8d1f"
      },
      "source": [
        "# We train the model\n",
        "history = model.fit(\n",
        "    train_gen, \n",
        "    epochs=5, \n",
        "    validation_data=test_gen, \n",
        "    verbose=2,\n",
        "    shuffle=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "205/205 - 86s - loss: 0.9441 - acc: 0.7503 - val_loss: 0.7121 - val_acc: 0.8063\n",
            "Epoch 2/5\n",
            "205/205 - 86s - loss: 0.5660 - acc: 0.8513 - val_loss: 0.5926 - val_acc: 0.8200\n",
            "Epoch 3/5\n",
            "205/205 - 84s - loss: 0.3976 - acc: 0.8954 - val_loss: 0.5714 - val_acc: 0.8165\n",
            "Epoch 4/5\n",
            "205/205 - 84s - loss: 0.2900 - acc: 0.9246 - val_loss: 0.5637 - val_acc: 0.8110\n",
            "Epoch 5/5\n",
            "205/205 - 82s - loss: 0.2216 - acc: 0.9418 - val_loss: 0.6085 - val_acc: 0.7985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQTpRH7QaDtN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "c98e60f3-df40-4c6a-d6e0-a7420d00096e"
      },
      "source": [
        "sg.utils.plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c3rEttLglev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "0cf28ef7-1109-43e5-f30a-7b33939950d6"
      },
      "source": [
        "# We evaluate the best model on our test set\n",
        "test_metrics = model.evaluate(test_gen)\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "52/52 [==============================] - 16s 305ms/step - loss: 0.6119 - acc: 0.7934\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.6119\n",
            "\tacc: 0.7934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK783RhnOHrQ",
        "colab_type": "text"
      },
      "source": [
        "### We get a validation accuracy of **79.34% !** Which is better than our previous bert model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaJNs7nSgxxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We make predictions on a new dataset\n",
        "all_nodes = data_dic[\"test\"].index\n",
        "all_mapper = generator.flow(all_nodes)\n",
        "all_predictions = model.predict(all_mapper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS-TnTYmgyYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09b8a182-db84-45f5-e203-e2d502791f7f"
      },
      "source": [
        "# As these data are under the shape of the softmax output layers, we need to inverse_transform function to get them under the shape of original categories.\n",
        "node_predictions = target_encoding.inverse_transform(all_predictions)\n",
        "node_predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 2, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7RH0CKwg7O3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We download the new predictions under a csv format\n",
        "from google.colab import files\n",
        "\n",
        "submit = pd.DataFrame(data=node_predictions , index = data_dic[\"test\"].index, columns = [\"label\"])\n",
        "submit.to_csv('Graphsage_1.csv') \n",
        "files.download('Graphsage_1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCV7D7gGk1go",
        "colab_type": "text"
      },
      "source": [
        "# Graph Attention Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RztDoBtUk2ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stellargraph.layer import GAT\n",
        "from stellargraph.mapper import FullBatchNodeGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lfE3SR5no7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use the FullBatchNodeGenerator to feed the node features and the adjency matrix to our keras model.\n",
        "generator = FullBatchNodeGenerator(G, method=\"gat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkUPw2FHnw8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen = generator.flow(train_subjects.index, train_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTauhH9Un0fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We specify our hyperparameters.\n",
        "# We use a GAT with 2 layers.\n",
        "# The first layer has 64 dimensional hidden nodes \n",
        "# the second layer has 5 class classification output.\n",
        "# We use 8 attention heads in our GAT.\n",
        "# We add dropout and attention dropout\n",
        "gat = GAT(\n",
        "    layer_sizes=[64, train_targets.shape[1]],\n",
        "    activations=[\"relu\", \"softmax\"],\n",
        "    attn_heads=8,\n",
        "    generator=generator,\n",
        "    in_dropout=0.5,\n",
        "    attn_dropout=0.5,\n",
        "    normalize=None,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfENuW1hn3FJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create input and output tensor of the GAT model \n",
        "x_inp, predictions = gat.in_out_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kPQUwT7n-Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create the keras model\n",
        "model = Model(inputs=x_inp, outputs=predictions)\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adamax(lr=0.01),\n",
        "    loss=losses.categorical_crossentropy,\n",
        "    metrics=[\"acc\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKrSglBloA3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use test_gen to calculate our validation loss anc accuracy\n",
        "test_gen = generator.flow(test_subjects.index, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5cz38L-oeA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create callbals for early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.makedirs(\"logs\")\n",
        "es_callback = EarlyStopping(\n",
        "    monitor=\"val_acc\", patience=20\n",
        ")  # patience is the number of epochs to wait before early stopping in case of no further improvement\n",
        "mc_callback = ModelCheckpoint(\n",
        "    \"logs/best_model.h5\", monitor=\"val_acc\", save_best_only=True, save_weights_only=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obt5eSuEoh6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bf96c60-d2f9-4d6e-ef4a-a4e006bcd3d2"
      },
      "source": [
        "# We train the model\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=300,\n",
        "    validation_data=test_gen,\n",
        "    verbose=2,\n",
        "    shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n",
        "    callbacks=[es_callback, mc_callback],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 - 1s - loss: 1.6125 - acc: 0.1958 - val_loss: 1.4063 - val_acc: 0.6886\n",
            "Epoch 2/300\n",
            "1/1 - 1s - loss: 1.4098 - acc: 0.5737 - val_loss: 1.2023 - val_acc: 0.7414\n",
            "Epoch 3/300\n",
            "1/1 - 1s - loss: 1.2490 - acc: 0.6078 - val_loss: 1.0094 - val_acc: 0.7723\n",
            "Epoch 4/300\n",
            "1/1 - 1s - loss: 1.1015 - acc: 0.6442 - val_loss: 0.8540 - val_acc: 0.7919\n",
            "Epoch 5/300\n",
            "1/1 - 1s - loss: 0.9889 - acc: 0.6739 - val_loss: 0.7451 - val_acc: 0.7985\n",
            "Epoch 6/300\n",
            "1/1 - 1s - loss: 0.9137 - acc: 0.6957 - val_loss: 0.6736 - val_acc: 0.8040\n",
            "Epoch 7/300\n",
            "1/1 - 1s - loss: 0.8725 - acc: 0.7099 - val_loss: 0.6285 - val_acc: 0.8079\n",
            "Epoch 8/300\n",
            "1/1 - 1s - loss: 0.8455 - acc: 0.7095 - val_loss: 0.5971 - val_acc: 0.8091\n",
            "Epoch 9/300\n",
            "1/1 - 1s - loss: 0.8297 - acc: 0.7357 - val_loss: 0.5738 - val_acc: 0.8114\n",
            "Epoch 10/300\n",
            "1/1 - 1s - loss: 0.8458 - acc: 0.7383 - val_loss: 0.5523 - val_acc: 0.8169\n",
            "Epoch 11/300\n",
            "1/1 - 1s - loss: 0.8368 - acc: 0.7327 - val_loss: 0.5372 - val_acc: 0.8220\n",
            "Epoch 12/300\n",
            "1/1 - 1s - loss: 0.8121 - acc: 0.7449 - val_loss: 0.5276 - val_acc: 0.8232\n",
            "Epoch 13/300\n",
            "1/1 - 1s - loss: 0.8209 - acc: 0.7445 - val_loss: 0.5219 - val_acc: 0.8224\n",
            "Epoch 14/300\n",
            "1/1 - 1s - loss: 0.7915 - acc: 0.7322 - val_loss: 0.5180 - val_acc: 0.8228\n",
            "Epoch 15/300\n",
            "1/1 - 1s - loss: 0.7977 - acc: 0.7359 - val_loss: 0.5156 - val_acc: 0.8232\n",
            "Epoch 16/300\n",
            "1/1 - 1s - loss: 0.7824 - acc: 0.7350 - val_loss: 0.5134 - val_acc: 0.8236\n",
            "Epoch 17/300\n",
            "1/1 - 1s - loss: 0.7665 - acc: 0.7391 - val_loss: 0.5127 - val_acc: 0.8251\n",
            "Epoch 18/300\n",
            "1/1 - 1s - loss: 0.7613 - acc: 0.7422 - val_loss: 0.5134 - val_acc: 0.8259\n",
            "Epoch 19/300\n",
            "1/1 - 1s - loss: 0.7415 - acc: 0.7471 - val_loss: 0.5144 - val_acc: 0.8275\n",
            "Epoch 20/300\n",
            "1/1 - 1s - loss: 0.7415 - acc: 0.7461 - val_loss: 0.5154 - val_acc: 0.8279\n",
            "Epoch 21/300\n",
            "1/1 - 1s - loss: 0.7440 - acc: 0.7383 - val_loss: 0.5169 - val_acc: 0.8267\n",
            "Epoch 22/300\n",
            "1/1 - 1s - loss: 0.7246 - acc: 0.7463 - val_loss: 0.5177 - val_acc: 0.8294\n",
            "Epoch 23/300\n",
            "1/1 - 1s - loss: 0.7280 - acc: 0.7489 - val_loss: 0.5173 - val_acc: 0.8282\n",
            "Epoch 24/300\n",
            "1/1 - 1s - loss: 0.7283 - acc: 0.7432 - val_loss: 0.5163 - val_acc: 0.8275\n",
            "Epoch 25/300\n",
            "1/1 - 1s - loss: 0.7135 - acc: 0.7529 - val_loss: 0.5156 - val_acc: 0.8290\n",
            "Epoch 26/300\n",
            "1/1 - 1s - loss: 0.7186 - acc: 0.7556 - val_loss: 0.5153 - val_acc: 0.8286\n",
            "Epoch 27/300\n",
            "1/1 - 1s - loss: 0.7123 - acc: 0.7529 - val_loss: 0.5152 - val_acc: 0.8302\n",
            "Epoch 28/300\n",
            "1/1 - 1s - loss: 0.6970 - acc: 0.7545 - val_loss: 0.5152 - val_acc: 0.8302\n",
            "Epoch 29/300\n",
            "1/1 - 1s - loss: 0.6922 - acc: 0.7629 - val_loss: 0.5157 - val_acc: 0.8286\n",
            "Epoch 30/300\n",
            "1/1 - 1s - loss: 0.6955 - acc: 0.7553 - val_loss: 0.5159 - val_acc: 0.8318\n",
            "Epoch 31/300\n",
            "1/1 - 1s - loss: 0.6885 - acc: 0.7621 - val_loss: 0.5157 - val_acc: 0.8326\n",
            "Epoch 32/300\n",
            "1/1 - 1s - loss: 0.6854 - acc: 0.7587 - val_loss: 0.5144 - val_acc: 0.8326\n",
            "Epoch 33/300\n",
            "1/1 - 1s - loss: 0.6798 - acc: 0.7649 - val_loss: 0.5127 - val_acc: 0.8326\n",
            "Epoch 34/300\n",
            "1/1 - 1s - loss: 0.6843 - acc: 0.7637 - val_loss: 0.5102 - val_acc: 0.8333\n",
            "Epoch 35/300\n",
            "1/1 - 1s - loss: 0.6708 - acc: 0.7626 - val_loss: 0.5078 - val_acc: 0.8329\n",
            "Epoch 36/300\n",
            "1/1 - 1s - loss: 0.6866 - acc: 0.7550 - val_loss: 0.5056 - val_acc: 0.8314\n",
            "Epoch 37/300\n",
            "1/1 - 1s - loss: 0.6773 - acc: 0.7616 - val_loss: 0.5033 - val_acc: 0.8314\n",
            "Epoch 38/300\n",
            "1/1 - 1s - loss: 0.6616 - acc: 0.7703 - val_loss: 0.5012 - val_acc: 0.8326\n",
            "Epoch 39/300\n",
            "1/1 - 1s - loss: 0.6847 - acc: 0.7629 - val_loss: 0.4996 - val_acc: 0.8322\n",
            "Epoch 40/300\n",
            "1/1 - 1s - loss: 0.6644 - acc: 0.7630 - val_loss: 0.4984 - val_acc: 0.8326\n",
            "Epoch 41/300\n",
            "1/1 - 1s - loss: 0.6615 - acc: 0.7646 - val_loss: 0.4975 - val_acc: 0.8329\n",
            "Epoch 42/300\n",
            "1/1 - 1s - loss: 0.6624 - acc: 0.7632 - val_loss: 0.4970 - val_acc: 0.8345\n",
            "Epoch 43/300\n",
            "1/1 - 1s - loss: 0.6527 - acc: 0.7686 - val_loss: 0.4965 - val_acc: 0.8349\n",
            "Epoch 44/300\n",
            "1/1 - 1s - loss: 0.6628 - acc: 0.7658 - val_loss: 0.4956 - val_acc: 0.8345\n",
            "Epoch 45/300\n",
            "1/1 - 1s - loss: 0.6631 - acc: 0.7695 - val_loss: 0.4942 - val_acc: 0.8361\n",
            "Epoch 46/300\n",
            "1/1 - 1s - loss: 0.6572 - acc: 0.7663 - val_loss: 0.4931 - val_acc: 0.8361\n",
            "Epoch 47/300\n",
            "1/1 - 1s - loss: 0.6416 - acc: 0.7781 - val_loss: 0.4919 - val_acc: 0.8369\n",
            "Epoch 48/300\n",
            "1/1 - 1s - loss: 0.6400 - acc: 0.7718 - val_loss: 0.4904 - val_acc: 0.8365\n",
            "Epoch 49/300\n",
            "1/1 - 1s - loss: 0.6529 - acc: 0.7652 - val_loss: 0.4892 - val_acc: 0.8361\n",
            "Epoch 50/300\n",
            "1/1 - 1s - loss: 0.6463 - acc: 0.7706 - val_loss: 0.4883 - val_acc: 0.8365\n",
            "Epoch 51/300\n",
            "1/1 - 1s - loss: 0.6491 - acc: 0.7639 - val_loss: 0.4876 - val_acc: 0.8353\n",
            "Epoch 52/300\n",
            "1/1 - 1s - loss: 0.6416 - acc: 0.7703 - val_loss: 0.4874 - val_acc: 0.8345\n",
            "Epoch 53/300\n",
            "1/1 - 1s - loss: 0.6287 - acc: 0.7702 - val_loss: 0.4871 - val_acc: 0.8345\n",
            "Epoch 54/300\n",
            "1/1 - 1s - loss: 0.6253 - acc: 0.7741 - val_loss: 0.4867 - val_acc: 0.8329\n",
            "Epoch 55/300\n",
            "1/1 - 1s - loss: 0.6169 - acc: 0.7802 - val_loss: 0.4863 - val_acc: 0.8345\n",
            "Epoch 56/300\n",
            "1/1 - 1s - loss: 0.6257 - acc: 0.7775 - val_loss: 0.4864 - val_acc: 0.8341\n",
            "Epoch 57/300\n",
            "1/1 - 1s - loss: 0.6199 - acc: 0.7756 - val_loss: 0.4862 - val_acc: 0.8349\n",
            "Epoch 58/300\n",
            "1/1 - 1s - loss: 0.6165 - acc: 0.7717 - val_loss: 0.4857 - val_acc: 0.8341\n",
            "Epoch 59/300\n",
            "1/1 - 1s - loss: 0.6278 - acc: 0.7751 - val_loss: 0.4853 - val_acc: 0.8337\n",
            "Epoch 60/300\n",
            "1/1 - 1s - loss: 0.6171 - acc: 0.7787 - val_loss: 0.4849 - val_acc: 0.8333\n",
            "Epoch 61/300\n",
            "1/1 - 1s - loss: 0.6112 - acc: 0.7779 - val_loss: 0.4847 - val_acc: 0.8322\n",
            "Epoch 62/300\n",
            "1/1 - 1s - loss: 0.6048 - acc: 0.7817 - val_loss: 0.4849 - val_acc: 0.8318\n",
            "Epoch 63/300\n",
            "1/1 - 1s - loss: 0.6066 - acc: 0.7866 - val_loss: 0.4849 - val_acc: 0.8322\n",
            "Epoch 64/300\n",
            "1/1 - 1s - loss: 0.6057 - acc: 0.7781 - val_loss: 0.4854 - val_acc: 0.8310\n",
            "Epoch 65/300\n",
            "1/1 - 1s - loss: 0.6117 - acc: 0.7723 - val_loss: 0.4856 - val_acc: 0.8306\n",
            "Epoch 66/300\n",
            "1/1 - 1s - loss: 0.6116 - acc: 0.7819 - val_loss: 0.4859 - val_acc: 0.8314\n",
            "Epoch 67/300\n",
            "1/1 - 1s - loss: 0.6063 - acc: 0.7790 - val_loss: 0.4855 - val_acc: 0.8314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuRsdgnppe9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "ceb71dc1-7483-4236-b2c1-9f1c876a27e1"
      },
      "source": [
        "sg.utils.plot_history(history)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAI4CAYAAACV/7uiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeXycZb3//9c1k31tmknSJmmbdKG0paWFUJQdBQVB0KOyyAHxh2wHDhzxdw4IgogHjxyPSlVQCyKCQgXEQ0EUBVkOCKUpLS3dm67plqTNPklmu75/3EmaZSZN2mTmTvt+Ph55JHPPnbk/M2nnPdd1X/d1GWstIiIiMrp4El2AiIiIDJ0CXEREZBRSgIuIiIxCCnAREZFRSAEuIiIyCiUluoDD5fP5bFlZWaLLEBERGRHLli2rs9YW9N0+6gO8rKyMysrKRJchIiIyIowx26JtVxe6iIjIKKQAFxERGYUU4CIiIqOQAlxERGQUUoCLiIiMQgpwERGRUUgBLiIiMgopwEVEREYhBbiIiMgoNOpnYhMRGZJQAELtEA5COND5FYRIEDxJ4E0Gb0rnV7Kzra0BWvZCa63zvaXG+Tkc6P/41jrbAy0QaO386vzZ2t6P3X2svt87f8b0rzMcAOOB9DGQnud8pXX+nJYLKZmQktX5vfMrOd15rL483s7jDFI46Bzb4x349e16vsE28CY59SRnOF8etRuHiwJc5EgSbIP2JucNNOjvER5+sJEhPJCFUMeB3+0ZQvQJIU9n6KTlQGYBZBVBVqHzlZoDJkpwdAmHoL0R2uqhvcH5HvRHDzPjcfbvCrNIqH+w9fw52H4gcLuDtxY6Gg/3VXak5kBSavT7ktJ6hGcG5JRCSgYYb/R6g23OB4i+220EvKn9A96GYd+mztet0fmbHPLzyO38exVBVuffLz3PeeyuDytdX12vnfH0/5ATanf+rUSCAx8vOdN5LZLSYn9w6fuzJ9l5Lbs+sPT88JKS1blvUv8PP9H+jWAO/H5y+qG/bi6gABcBJxjqt0K448Abw6H+5w4HnaDrejPxeA+EmLVOEHa9IbZ2fm9rIOqbsPFAps95U80sPBCM3hRo2AZ7PoK9H8GeVc73+q2H+AIMQtcbr/H0D6FYvKmQmh09xEMd0NE0cvVCj3AqhHGzndcxw+f8bfu+6Xu8EIl0BmnP5xd0WrbdIVfo/C2S00a29sGKhJ3Xsa3e+XfU/cGtx1fQH+N3Q9Bad+BDzp5V0PKa83ipOQeea9EsmHK289pB5weoYO/XqPtDS8aBHoDkjM4PKH3qCbQ4LfW+H8ZCHQf+//T8G3S16g/3w0pf3tQevRg50XtDjMf5gNWrR6WzdyFWLV94FCafNXx1xqAAl9El2NajRbC3MwBrnTeoaJ+2kzP6f2pPG+O8WdVtgNr1ztf+qv5BlJR2YP+03M43pj7dk5HwgVq6Atm/r0/R5sCbQSQMobbDfx28qc6Hja7Hz58C4+fC3CsgY2yfLtQsJ7DMAN2e0SSl9n4jjtX1aa3zenc09W6xtdZA857Y4eFN6d392/U3Ss6I3hqNhKO3tDzJTq39Wm6pkJQytOc8Gnm8B16/4RIOOa+z20QiTi9AW2dvTVu9E6aRYO8PBF29F13/Ljw9QtlGOnt7ejxGe4PTc9XVG9LzQ0UkfOD/QEqW8yEuJcv5AGdi/J/ILIzLy+HCv5Ackax1/iP0/A8Wauvs2qzp3yLt7rrtex6xJfrjR+t+8yQ54dFWH72VaDyQVwa+6XDMp5zvKRkHWjJd/7nb6qGj2fkP3rynd03Gc6CVkj8FJn3c+Q+emt0ZQMHerRXo0c3c9b2zyzLam0E4CP66A92/XQHZ0QhjpzitysIZzhtMohjjBGWSz+ktKJqVuFpkeLgxvMH5ENn9YaU80dUknEv/ShIXDTtgw1+cFmjPsOo6FxkK9B4I0/XlTenddRaty7E7qHu0igfT9ZWW6wRcao5zrJyS3sfO9HV2JfcIwMyCgQfiWOt8qu753NLznAB0SzdoLB4v5JY6XyIiPSjAjyaRCOz6ANb/2QnuvR8529PGON2u6XnO97GTnZ+9KZ3nrnoMYmpvcrpuu1q5SSngzTrQ4o06ECWpcyBOn/NLSamdrdHCA+d3Yw0KOhzGdHZ/Z0BuyfA/vohIAijAj3ShDtj8Jqxd7IR2a61zLnTix+Dc78L08yF/6sAjhUVExHUU4EeigB82vQprX3RCu6MJUrJh2rlOYE89x2lpi4jIqKUAH82C7c5lQ/VbYP9m2L/FGU29/T1n8Fb6WJh5Ecy4yLmkYSS6p0VEJCEU4G4VbHfOV1cvdUY+RxsZ3VrT+3dSc2FsGRx/uRPck05z72hSERE5LHF9dzfGnAcsALzAo9ba7/e5fyLwG2BM5z53WGtfjmeNCdNaB9vfdVrPO5bArhUHZjRKye68hrnzetnCY52BZzklzoCzseUHBp7pXLaIyFEhbgFujPECDwHnAtXAUmPMYmvtmh67fQt4xlr7c2PMTOBloCxeNcZd/TZY9xKsWeyEdtcUlcUnwMf/BSZ8DCacDJn5ia5URERcJp4t8PnAJmvtZgBjzCLgYqBngFsgp/PnXGBXHOuLj7pNsPYFJ7R3r3C2Fc2Gs+6AyWdD8VydqxYRkYOKZ4CXADt63K4GTu6zz73AX40x/wpkAudEeyBjzHXAdQATJ04c9kKHXaAVVv8RPniis6UNlFTAOd+BGZ91ZvASEREZAreNcLoceNxa+0NjzMeBJ40xx1nbexkla+1CYCFARUXFMM5sP4yshV3LndBe9RwEmsF3jHPt9XH/pJm1RETksMQzwHcCE3rcLu3c1tM1wHkA1tp3jTFpgA/oM9za5fZVwbNXw56VkJQOsz4PJ37FOZ+tQWYiIjIM4hngS4FpxphynOC+DPhyn322A58EHjfGzADSgNo41nj4WvfB777oXPJ1wY9g9hed+b1FRESGUdwC3FobMsbcDLyCc4nYY9ba1caY+4BKa+1i4BvAI8aYr+MMaLvaWuvOLvJogu2w6HJo3AlXvwQT5ie6IhGRo97uxjbeWF/L6+tqaG4P8dVTyzh3ZhFmlPeImtGUj9FUVFTYysrKRJfhLBTy3Fdhzf/Cl34Dsz6X6IpERI5KgVCE5dvreX19LW+sr2HdnmYASsak4/HAjv1tzByfwy2fnManZhbh8QwtyK217GxooyA7ldQk70g8hV6MMcustRV9t7ttENvo9dq9Tnif+12Ft4gcUSIRy/tb9/PmhlryM1OYWpjF1MIsinPThxx+hyIUjhCKWFKTPFFbzXUtHXywrZ5l2+tZvq2BD6sb6AhFSPIYTioby52fOZazphcyrTCLcMTywopd/Oz1Tdzw22UcOy6bWz45jfNmjRvUc/loZyPffWkNS7bsx2OgLD+TaUVZHFOUzbSibI4pymKyL4uUJM9IvBS9qAU+HJb+Cv50G1RcAxf8UAPVRKSXmuZ2/uvldfgDIf7phFLOnl4Ylzf4w7V+TzN/XL6TxSt2squxHa/HEI4cyIz0ZC9TCjOZWpBFaV4G43LTGJ+bxrjcNIpz0xmTkTzkbuqu1u2HOxpZsaOeFTsaWLWzkfagE8iZqUlkdX2lJVHX0sG2fX4Akr2GWcW5nDgpj5PK8jh1qo/stOSoxwmFI7y0cjc/+ftGNte2MrUwi8/PK+Gi44uZMDaj3/57m9r5wSvr+cMH1eRlpHDNaeW0B8Ns3NvChppmtu3zd782v7zyRD49a9yQnvdAYrXAFeCHa8Nf4elLYeq5cNlTmntcRHp5ZfUevvn8Klo7QmSnJVPX0sHYzBQunlvMF08sZVaxewa5hsIR1uxu4p1N+3hhxU7W7WnG6zGcMc3H5+aVcO7MItqDETbVtLCxpplNNS1sqmlhc20ruxvbiPSJk9QkDzPG51AxKY+KsjxOnDSWguzeE1XVNnewamcDq6qbWLWzgRU7Gqlr6QAgJcnDrOIc5k4Ygy8rldaOEC1dX+0hWgMhslKTOHFSHidMzOO4klzSkofWpR2OWF5auYsn3t3Gsm31AMydMIbPHl/MhXPGk5OWzMK3NvOLN6sIRyxfPbWMmz4xlZw+Hww6QmG21LWyYW8Lp0zJx5c1fBNyKcBHQu16WHi2MxHLV/8MqVmJqUNEXKelI8R9L67mmcpqZhXnsOCyuZTlZ/J/G+t4blk1f1uzl0A4wozxOXzpxFI+N6+EsZkpB33cjlCYjlCkX4AMpD0Y7v7ZGPAYgwFaA2GWb69n2bZ6Krc6rd22zn3nThjD5+eVcMGc8YMKo1A4Ql1LgN2NbexpbGd3Yzs7G9pYWd3Ah9WNBELOdB5l+RmcMDGPlo4Qq3Y2sruxvbuuKQVZzCnJZe7EMcydMIZjx+XEtaeiut7PSyt3s3jFLtbsbsIYyElLprEtyGdmj+OO82YwMb9/63ykKcBHwm+/CNXvw78sgZzxialB5Ajxj6o6fvy3DWysaeGLJ5TylVPKonZljoS9Te38o6qOvIyUAc/tNvgDrNjRwPLtTrduTloSs4pzmVWcw8ziHMZkOAFcuXU/X39mBTvr27jxrCnc+slj+gVRgz/Aix/u4rll1XxY3UiK18O5M4u45KQJnDbVh7fH8Vs6QryxvoZXVu/l9XU1tAXDnDOjkMvmT+SMaQW99u3S1B7kpQ938+yyHSzf3jDg8/cYulvKJ5aN5aSyPMbnph/KSxlVRyjMRzubWLZtP5Vb61m+o4HstCTmlORyXEkuc0rHMLM4h6xU9/Rgbqpp4cUPd7GxppmrTylnfvnYhNWiAB9um9+AJy52Bq2dekv8jy8yglo6QoTCTitvpAcpVW7dzw//uoF3N+9jXE4ax0/I5bW1NUSs5dOzxvH/nVZOxaS8QzqXOtDvbKlr5ZXVe/jLR3tYsaN3wGWkeJlSkMW0wiwm5Weyfb+f5Tvq2VzbCjiBN7Uwi6a2EHua2rt/r2RMOuW+TP5RVUdJXjo/vmQuFWUHf+Nft6eJZ5ZW88fl1dT7gxTnpvHFigkU56bx1zV7eXtTHYFQhPzMFM6dWUR2WhLPf7CTfa0BSsakc+lJE7ikYgKF2am8t3kfz1Tu4C+r99AejDCtMIvzZ48nPdlLpPP9PhKxWCDJa5hTMoa5E8e4KjylNwX4cIpEYOGZzmQtNy+F5LT4Hl+Oai0dId7fso+KsrGD7kb1B0KkJXkHDONAKMIb62v4wwfV/H1dDcGwxWNgTEYKYzKSyctIIS8jmdz0FHLSk8hNTyYnLZmc9GRy0pKYWZxDad7gW8wrqxv44V838OaGWnxZqfzLWVP48skTSUv2sruxjSff3cZT72+nwR9kdkkuXzihhPQULxHrzFQcsU4IBUIRaps7qGlud743OT83tAXJTU8mPzMFX1YqvqxU8rNSSPF6eGtjLRv2tgAwuySXT88q4qzphbR2hNhU28LGvS1U1Trnd3c3tpOfmcK8iXnMmziGeRPHMKf0QODta+lgze4mVu9qYs2uJtbvaebEsjzu/MyMIYdiRyjMq2tq+H3lDv5vYy3WQmleOp+eNY5PzxrHiZPyulvbgVCEv63Zy9Pvb+ftTXV4DPiyUqlp7iA7LYmLji/mSxUTOL40d9Rf73y0U4APp5XPwPPXwj89AnMuie+x5ajV3B7kN//YyqNvb6HBHyQrNYlLT5rAV08tixqc4YjlrQ21/G7Jdv6+bi+ZKUkcV5LL7NJcZpc4X5PyM1i1s5E/LKtm8Ye7qPcH8WWlcvHcYorHpNPgD1DvD1DfGnS++4M0tQVpbAvS0hHqdTyPgfOPG8/XTi9n3sS8qM+hIxTmb2v2suj9Hby9qY68jGRuOHMKV358Ehkp/cOuLRDm+eXVPPb2Fqo6W7/RJHsNBVmpFOSkUZidSmF2KmMynHOX+1oC7GsJUNfawb6WAC0dIU4qy+PTs8bxqVnjKBkzcFdxezAc8/KlkbS7sY2mthDHFGUd9Njb9rWyaOkONte28JnZ4/n0rHFDHswl7qUAHy7BdvhZBWSMhWvfAI/7LwWR0a2pPcjj72zlV29vobEtyCeOLeSSilL+8tEeXly5G4DzjxvHtadP5vgJY6hpaueZyh08/f4Odja04ctK4XNzS2gPhVlV3cja3c0Ews6AorRkD+3BCClJHj41s4gvnFDK6dN8JHkP/u86FI7Q0hGisS1IvT/Inz/azVNLttPcHqJiUh5fO30y584swusxbNjbzO+X7uD5D5wu4pIx6Xz55Il85ZSyQbVSIxHL3uZ2rO09CMsYQ7LXDKmr/2Bd6yJuowAfLu/8BP52N1y1GCafGb/jyqjTHnQuK9lc20pVrdMl2+APUu7LZEpBJlMKsphSmEVhdmp3oITCEfa1Bqht7qC2pYPl2xt4/J0tNLWHOGdGIbd8chpzSsd0H2NXQxuP/2MrTy/ZTnOH01rbXNtKKGI5dWo+X54/iXNnFvUaQBUIRdiwt5mPdjaydncT08flcMGc8eSmD35UcywtHSGeWbqDx97ZQnV9G5PyMxibmcLy7Q0kew2fmjmOS0+awKl9BmmJSGwK8OHg3w8/meusKnbFs/E5powqze1BHnq9ij+t2kV1fRs9/3uVjEknNz2ZbftaaQ0cuKwnKzWJopxU6v1ON3Xf/5Lnzizi1k9O47iS2NcLN7cH+f3SHby8ajcVZWO5fP5Eyn2Zw/30Bi0UjvDK6r089s4WWjtCfOGEUv7phBLyh/HaWJGjhQJ8OLxyF7z3MNzwDhTNjM8xJWFC4Qg76tvYXNtCVmoSFWVjY7YawxHLM5U7+OFf11PXEuCcGYUcV5LrtLILsij3ZZKe4pyTtNayt6mju1VeVdNCTbMzuYcvK5WC7NTu7yVj0hmXq0GSIkczzYV+uOq3wvsLYe4VCm+X2LHfz9/X1fDOpjosdI6ITuoeGZ2bnszHp+QfdJASOOdY39pYy9Kt+6mqcbq8t+5rJRg+8AHXl5XK+ceN44I54zmpR5i/W7WP+15aw9rdTVRMyuOxq0/q1c3dlzGGcZ3TTZ461XfYr4OIHJ0U4IP12n1gvHD2XYmuZMQ1tQe5d/FqXl9XQ5LXQ4rXQ7LXkOT1kOz1MGlsBvd8dibFgwjG4RQMR1i2rZ7X19Xw93U1bKxxLgOaODaDzNQkmtqaaGoL0txjdLTXY7hg9niuO2Ny1C7oYDjCix/u4hdvVrFhbwtJHsOk/AwmF2TxyRlFTCnIZHJBFnsa2/nTql08u2wHT763jYJsJ8z3NrXzyuq9lIxJ52dfnscFs8drgJSIxIW60Adj13JYeBac8e/wiW+N7LESbNm2em5dtJzdje1cPLeY1CQvwXCk+ysQivBu1T6SvB4e+MJszjtuZGaga24Psn5PM+v2NLNuTxPrdjs/t3SESPYa5peP5ezphXzi2EImF/SewjYcsbS0h6hpPjAau6UjxKlT87n29MmceUwBbcEwv1+6g0f/bws7G9qYXpTNDWdN5oLZxQNO3egPhPj7uhpeXrWbv6+rwWMMN509lWtOK9dlOyIyInQO/HC8dBt8+DR8Yz2k5YzssYZRc3uw1wjoMekpnHfcuKjTU4Yjlodf38SDr21kfG4aCy6bx4mTol/Lu7WulVsWLWdldSNXnDyRuy+ceVjhZa1lc10r72/Zz/tb9lO5bT879rd135+dmsSx47M5dlwOp0zJ57RpsVcYiqapPcjTS7bz2Dtb2NvUwbTCLOpaOqj3B5lfNpYbz5rCWdMLhtxy9gdCWAuZmsFKREaQAvxQRSLwo2Nh4sfgkidG7jjD5LfvbeNPK3dTVesMjOrScxnA40tzuWDOeD4zezyleRnsbGjj64tW8P7W/Vw8t5jvfu64g87wFQhF+OFf1/PLtzZzTFEWP738BKaPy+6+v6apnQ+2N7B8Rz1VNa2kJXvITksiMyWpezlAgOU76nl/y37qWgKAc575pDJnVaFjx2Vz7PgcinPThqVbOhByust/u2QbvqxUbjhzMidOStz8xiIig6EAP1Tbl8Bjn4Iv/Apmf3HkjjMM3q3ax+WPvMcxRVnMKR3TOQI6kymFWUwcm8HuhnZe/mg3f1q5m1U7GwE4fsIYttS2EI5Yvvu54/inE0qHdMy3NtRy2zMf0twe5KqPT2JXYzsrtjews8FpQSd7DeW+TIJhS0tHiNaOEP4el1CVjEnn5PKxzO/8Kvdl6hyyiEgPCvBD9cpdzujzf69ydfd5WyDM+QvewgJ/ufWM7kuWYtm2r5WXV+3h5VW7yUlP4nufn82k/EO7briupYP//9kPeWN9LSVj0jvni3bmjZ45Pqdf93o4YmkNhAiF7aCWTxQROZrpMrJDYS2sXQyTz3J1eAM8+OoGtu7z89S1Jx80vAEm5Wdy41lTuPGsKYd9bF9WKr+++iRaOkKDOjft9ZghrWUsIiL9aSLvgexZCQ3bYcZFia5kQCurG3jk/zZz+fwJnDIlMdcVG2OGNLBMREQOjwJ8IGtfBOOB6Z9JdCUxBcMR/uO5lRRkp3LH+TMSXY6IiMSJutAHsmYxTDoVMvMTXUlMv3yzinV7mll45YnDshiFiIiMDmqBx1K7HurWu7r7fFNNMz95bRMXzBnPp2aNS3Q5IiISRwrwWNa+6HyfcWFi64ghHLH8x3MryUj1cu9nZyW6HBERiTN1oceydjGUngQ5xQkt460NtextaiclyZmHPLlzXvLKrfV8sL2BH11yPAXZWqJRRORoowCPpn4b7P4Qzr0vYSWEwhG+8+IannxvW8x9zppewOfnlcSxKhERcYu4Brgx5jxgAeAFHrXWfr/P/T8Gzu68mQEUWmtjr8s4Uta95Hyf8dm4Hxqg0R/kpqc+4O1NdVx/xmT++WOTOhcTsc6CIuEI4Yjl+NIxmrVMROQoFbcAN8Z4gYeAc4FqYKkxZrG1dk3XPtbar/fY/1+BefGqr5e1L0LRbBg7Oe6H3lLXyjWPL2VHvZ///uIcLqmYEPcaRETE/eI5iG0+sMlau9laGwAWARcPsP/lwNNxqayn5r2w/b2EtL7/samOzz30Dg1tQX73tY8pvEVEJKZ4BngJsKPH7erObf0YYyYB5cDfY9x/nTGm0hhTWVtbO7xVrnsJsHEP8KeWbOeqx96nKCeVF246lfnlWiVLRERic+tlZJcBz1lrw9HutNYutNZWWGsrCgoKhvfIa1+EsVOgMH6zmr344S7u/OMqTpvm4w83nhJ1vW4REZGe4hngO4GefcKlnduiuYxEdJ/798PW/4OZF0GcBodt3NvM7X9YyYmT8lh4ZYXmExcRkUGJZ4AvBaYZY8qNMSk4Ib24707GmGOBPODdONbm2PAXiITi1n3e3B7k+t8uIyPFy0NfPoGUJLd2iIiIiNvELTGstSHgZuAVYC3wjLV2tTHmPmNMz/lKLwMW2UQsVG68UHY6FJ8w4oey1plJbds+Pz+9/ATG5aaN+DFFROTIYRKRk8OpoqLCVlZWJrqMIXvkrc3c//Ja7vzMsVx3xuGvyS0iIkcmY8wya21F3+3qs02AJZv38f2/rOP848Zx7enxv9ZcRERGPwV4nO1tauemp5YzKT+D//7iHM2kJiIih0RzocdRKBzhpt99gD8Q4qlrT9aIcxEROWQK8Dj63ZLtVG6rZ8FlczmmKDvR5YiIyCimLvQ4afAH+PGrGzhtqo+Ljk/sEqUiIjL6KcDj5MFXN9LUFuRbF87QeW8RETlsCvA42FTTzJPvbePLJ0/k2HE5iS5HRESOAArwOPjuS2vJSPFy27nTE12KiIgcIRTgI+z1dTW8uaGWWz85jbGZKYkuR0REjhAK8BEUDEf47p/WMNmXyVUfL0t0OSIicgRRgI+gJ9/dxubaVu66YIYWKhERkWGlVBkh+1sDPPjqBk6f5uMTxxYmuhwRETnCKMBHyI//toHWQJi7L5ypy8ZERGTYKcBHwKaaFn63ZBtXnDxRM66JiMiIUICPgF+8WUVKkodbPzkt0aWIiMgRSgE+zHY1tPG/y3dy2UkTyc9KTXQ5IiJyhFKAD7Nfvb0FC3zt9PJElyIiIkcwBfgwavAHePr97Vx8fDGleRmJLkdERI5gCvBh9MS72/AHwlx/5pRElyIiIkc4BfgwaQuEefwfW/nEsYVMH6eR5yIiMrIU4MPkmcod7G8NcONZan2LiMjIU4APg2A4wsK3NnPipDxOKhub6HJEROQooAAfBn9auZudDW3cqHPfIiISJwrww2St5RdvVjGtMEtznouISNwowA/TG+trWbenmRvOnILHoznPRUQkPhTgh+nnb1ZRnJvGRXOLE12KiIgcRRTgh2HZtnre37Kfa06fTLJXL6WIiMRPXFPHGHOeMWa9MWaTMeaOGPtcYoxZY4xZbYx5Kp71DdVPXtvI2MwULjtpQqJLERGRo0xSvA5kjPECDwHnAtXAUmPMYmvtmh77TAO+CZxqra03xrh2VNjy7fW8uaGW2887lszUuL2MIiIiQHxb4POBTdbazdbaALAIuLjPPtcCD1lr6wGstTVxrG9IFry2kbyMZK76+KRElyIiIkeheAZ4CbCjx+3qzm09HQMcY4x5xxjznjHmvGgPZIy5zhhTaYyprK2tHaFyY1uxo4E31tdy7RmT1foWEZGEcNvIqyRgGnAWcDnwiDFmTN+drLULrbUV1tqKgoKCOJfonPsek5HMVR8vi/uxRUREIL4BvhPoOdqrtHNbT9XAYmtt0Fq7BdiAE+iu8eGOBv6+roZrT59MllrfIiKSIPEM8KXANGNMuTEmBbgMWNxnn//FaX1jjPHhdKlvjmONB/WT1zaSm65z3yIiklhxC3BrbQi4GXgFWAs8Y61dbYy5zxhzUedurwD7jDFrgNeBf7fW7otXjQezqrqR19bVcO3p5WSnJSe6HBEROYrFtQ/YWvsy8HKfbff0+NkCt3V+uc6Cztb3V04pS3QpIiJylHPbIDbX+mhnI6+u3cs1p6n1LSIiiacAH6QFr20kJy2Jq08tS3QpIiIiCvDBWLOrib+t2cs1p00mR61vERFxAQX4ILy/xRlHd9l8zXkuIiLuoAAfhNZAGIDcdLW+RUTEHRTgg9AWCOMxkJqkl0tERNxBiTQIrYEQmSlJGGMSXYqIiAigAB+UtkCY9BRvoh+urMIAACAASURBVMsQERHppgAfhNZAWKuOiYiIqyjAB6EtECI9WS1wERFxDwX4ILR2hMlMVYCLiIh7KMAHwR8Mk56iLnQREXEPBfgg+DtCZGoQm4iIuIgCfBD8GoUuIiIuowAfBH/ndeAiIiJuMegAN8ZcbYy5JMr2S4wxVw1vWe7iD4TJUAtcRERcZCgt8NuB/VG21wF3DE857hOOWDpCETLUAhcRERcZSoCXAZuibN/ced8RyR8IAagFLiIirjKUAG8EyqNsnwK0DE857uPvXIksQ9eBi4iIiwwlwP8M/MAYM75rgzGmGHgAeHm4C3OL7gBXC1xERFxkKAH+H0AmUGWMqTTGVOJ0qWd23ndEau3o6kLXOXAREXGPQaeStbbWGDMPuAI4oXPzw8DT1tq2kSjODdqCaoGLiIj7DKlZaa1tB37V+XVUUAtcRETcaCjXgd9hjLkmyvZrjDFHbBd6m86Bi4iICw3lHPh1wPoo29cC1w9POe7T2hngmolNRETcZCgBXgxUR9m+CygZnnLcp63zOnDNhS4iIm4ylACvAWZH2T4H2DeYBzDGnGeMWW+M2WSM6Td7W+d0rbXGmBWdX18bQn0jorsFruvARUTERYbSL/w88GNjTLW1djmAMeYE4IfAcwf7ZWOMF3gIOBenJb/UGLPYWrumz66/t9bePIS6RlTXdeBpSQpwERFxj6G0wO/CCd5lxpg6Y0wdUInThX7nIH5/PrDJWrvZWhsAFgEXD7XgePN3hMhI8eLxmESXIiIi0m0o14G3AmcZYz4BnNi5eZm19u+DfIgSYEeP29XAyVH2+4Ix5gxgA/B1a+2OKPvEjT+olchERMR9hrQeuDEmDygCvEA6cJox5h5jzD3DVM+LQJm1dg7wN+A3Meq4rms2uNra2mE6dHROC1wj0EVExF0GnUzGmJOAvwAGyAFqgULAD+wG7jvIQ+wEJvS4Xdq5rZu1tudguEeB/472QNbahcBCgIqKCjvY53AotBa4iIi40VBa4D8A/gD4gDbgVGASsBxnrfCDWQpMM8aUG2NSgMuAxT136LlQCnARzjXmCaUAFxERNxpK3/Bc4EZrbcQYEwFSrLWbjTG3A48Bfxzol621IWPMzcArOF3wj1lrVxtj7gMqrbWLgVuMMRcBIWA/cPXQn9Lw8gfUhS4iIu4zlGQKA8HOn2twusPXAXU4LfGDsta+TJ+lR6219/T4+ZvAN4dQ04jzB8L4slITXYaIiEgvQwnwlTit8E3Ae8CdxhgPcC3Rp1g9IqgLXURE3GgoAX4/kNX5893An4A/4wxm++Iw1+Ua/kCIjFR1oYuIiLsM5TrwV3v8vBWYZYwZC9Rba0d0JHgi+QNhMpLVAhcREXc5rKaltXb/cBXiRpGIdQJcLXAREXGZIU3kcrRpD2ktcBERcScF+ABaO7rWAleAi4iIuyjAB9DWuRJZuq4DFxERl1GAD6A1EALUAhcREfdRgA/A390CV4CLiIi7KMAH4O9qgWsUuoiIuIwCfADdLXBdBy4iIi6jAB+AWuAiIuJWCvABdLXAdR24iIi4jQJ8AP4OBbiIiLiTAnwAB1rg6kIXERF3UYAPwB8IkZrkwesxiS5FRESkFwX4ALQWuIiIuJUCfACtgZC6z0VExJUU4ANoUwtcRERcSgE+gFatBS4iIi6lAB9AWyBEhmZhExERF1KAD6C1I0xmqgJcRETcRwE+gLZgWGuBi4iIKynAB9DaEdJa4CIi4koK8AG0BcJaC1xERFxJAR6DtZbWQIhMdaGLiIgLKcBj6AhFiFjUAhcREVeKa4AbY84zxqw3xmwyxtwxwH5fMMZYY0xFPOvrqWshE50DFxERN4pbgBtjvMBDwPnATOByY8zMKPtlA7cCS+JVWzT+QAjQSmQiIuJO8WyBzwc2WWs3W2sDwCLg4ij7fRd4AGiPY239dC8lquvARUTEheIZ4CXAjh63qzu3dTPGnABMsNb+aaAHMsZcZ4ypNMZU1tbWDn+l9FwLXAEuIiLu45pBbMYYD/Aj4BsH29dau9BaW2GtrSgoKBiRevwd6kIXERH3imeA7wQm9Lhd2rmtSzZwHPCGMWYr8DFgcaIGsqkFLiIibhbPAF8KTDPGlBtjUoDLgMVdd1prG621PmttmbW2DHgPuMhaWxnHGru1ahCbiIi4WNwC3FobAm4GXgHWAs9Ya1cbY+4zxlwUrzoGq00tcBERcbG4Ni+ttS8DL/fZdk+Mfc+KR02xtHZfB64WuIiIuI9rBrG5TVtnF7pmYhMRETdSgMfQGgiT7DWkJOklEhER91E6xdAWCJOerNa3iIi4kwI8htaOEJmpOv8tIiLupACPwR/UWuAiIuJeCvAY/B1aC1xERNxLAR6DP6AWuIiIuJcCPAZ/IKy1wEVExLUU4DH4AyFNoyoiIq6lAI/BHwhrGlUREXEtBXgMCnAREXEzBXgM/kCIDF0HLiIiLqUAjyIQihAMWzI0E5uIiLiUAjyK7qVE1QIXERGXUoBH4Q86K5HpHLiIiLiVAjyK1o7OFrgCXEREXEoBHkV3F7quAxcREZdSgEfRGnC60DUTm4iIuJUCPIquFrjmQhcREbdSgEfR3QLXKHQREXEpBXgU/q4WuK4DFxERl1KAR+HvUAtcRETcTQEehT+oy8hERMTdFOBR+DvCeAykJunlERERd1JCReGsRJaEMSbRpYiIiESlk7xR+AMhdZ+LSMJFIhGqq6tpbW1NdCkyQjIzMyktLcXjGXp7WgEehdYCFxE3qKurwxjD9OnTD+kNXtwtEomwc+dO6urqKCwsHPLvx/VfhDHmPGPMemPMJmPMHVHuv8EYs8oYs8IY87YxZmY86+vitMD12UZEEquhoYGioiKF9xHK4/FQVFREY2Pjof3+MNcTkzHGCzwEnA/MBC6PEtBPWWtnW2vnAv8N/Che9fWkFriIuEE4HCY5OTnRZcgISk5OJhQKHdLvxvNj3Xxgk7V2s7U2ACwCLu65g7W2qcfNTMDGsb5urYGw1gIXEVfQYNoj2+H8feOZUiXAjh63q4GT++5kjLkJuA1IAT4R7YGMMdcB1wFMnDhx2AttC4QYn5M27I8rIiIyXFx3YsVa+5C1dgpwO/CtGPsstNZWWGsrCgoKhr2G1o4wGanqQhcREfeKZ4DvBCb0uF3auS2WRcDnRrSiGNqCOgcuIuIWjz/+OElJOq3ZVzwDfCkwzRhTboxJAS4DFvfcwRgzrcfNC4CNcayvW2tHiEyNQhcROWTnnHMOV1999bA81qWXXsrOnQO1945OcUspa23IGHMz8ArgBR6z1q42xtwHVFprFwM3G2POAYJAPfCVeNXXJRyxdIQiWgtcRGSEBQIBUlJSDrpfeno66enpcahodInrOXBr7cvW2mOstVOstfd3brunM7yx1t5qrZ1lrZ1rrT3bWrs6nvWBcw04oBa4iMghuvrqq3nttdf4zW9+gzEGYwyPP/44xhh+97vf8ZnPfIbMzEzuvvturLVce+21TJkyhfT0dCZPnsydd95JR0dH9+P17ULvuv3OO+9wwgknkJGRwYknnsjSpUsT8XQTRinVR1vXWuBqgYuIy3znxdWs2dV08B2H2cziHL792VmD3n/BggVs3ryZ8ePHs2DBAgCampy6b7/9dh544AEeeughAKy1FBYW8tRTT1FUVMTKlSu5/vrrSU5O5jvf+U7MY0QiEb75zW+yYMECCgoK+PrXv84ll1zCxo0bj5rz5UfHsxyC1s4Az9QodBGRQ5Kbm0tKSgrp6emMGzcOgPb2dgCuv/56rrjiil7733///d0/l5WVUVVVxcMPPzxggFtrefDBBznhhBMAuPfee/nYxz5GVVUV06dPH+6n5EoK8D66utDTk/XSiIi7DKUV7Fbz58/vt+2RRx7h0UcfZevWrbS2thIKhYhEIgM+jjGG448/vvt2cXExAHv37j1qAtx114Enml8tcBGREZOZmdnr9rPPPstNN93EpZdeyssvv8zy5cu55557CAaDAz6Ox+PB6z3wPt01o9nBgv9IomZmH10BruvARUQOXUpKCuFw+KD7vfXWW8ybN4/bbrute9vWrVtHsLIjh1rgffg7nC50rUYmInLoysvLWbZsGVVVVdTV1cVsUU+fPp1Vq1bxwgsvUFVVxYIFC3j++efjXO3opADvQy1wEZHD941vfAOfz8fxxx9PQUEB77zzTtT9rr/+eq688kq++tWvMm/ePJYsWcK9994b32JHKWNtQhb8GjYVFRW2srJy2B7vyXe3cvcLq1l61zkUZKcO2+OKiAzV2rVrmTFjRqLLkBF2sL+zMWaZtbai73a1wPtQC1xEREYDBXgfXdeBpycrwEVExL0U4H20BUKkJ3vxeA59kXUREZGRpgDvozUQ1jXgIiLiegrwPtoCYc2DLiIirqcA70NrgYuIyGigAO+jLagWuIiIuJ8CvA+1wEVEZDRQgPfh1zlwEREZBRTgffgDYTIV4CIiCfX444+TlHSgN/SNN97AGEN1dfWAv2eM4be//e1hH//qq6/mnHPOOezHGUkK8D6cFri60EVE3OSUU05h9+7d3et+D5ff/va33UuR9rRgwQKeffbZYT3WcFNS9eEPhNQCFxFxmZSUFMaNGxe34+Xm5sbtWIdKLfAeIhFLWzCsedBFRA7DI488Qm5uLu3t7b22P/DAA0ycOJFwOMy1117LlClTSE9PZ/Lkydx55510dHTEfMxoXeivv/46c+bMIS0tjTlz5vD666/3+7277rqLGTNmkJGRwYQJE7jhhhtobGzsfswrr7wScLrejTFcffXVQP8udGst//M//8PkyZNJSUlhypQpPPjgg72OVVZWxj333MOtt97K2LFjKSoq4utf/zqhUGhoL+AgqQXeQ3sojLWQkaqXRURc6M93wJ5V8T/uuNlw/vcHvfsll1zCLbfcwgsvvMCll17avf2JJ57gn//5nzHGUFhYyFNPPUVRURErV67k+uuvJzk5me985zuDOsauXbu48MILueSSS1i0aBE7d+7k1ltv7bdfeno6CxcuZMKECVRVVXHTTTdxyy238Jvf/IZTTjmFn/3sZ9x8883s3r27e/9oHn74Ye6++24WLFjA2WefzWuvvca//du/kZ2dzTXXXNO9309/+lNuv/12lixZwvLly7niiis47rjjeu0zXJRUPWglMhGRw5ebm8vFF1/ME0880R3glZWVrFmzhueffx6Px8P999/fvX9ZWRlVVVU8/PDDgw7whx9+GJ/PxyOPPEJSUhIzZ87ke9/7Hp/97Gd77fetb32r13H+67/+i8suu4xf//rXpKSkdHeVH6x7/vvf/z7/+q//ynXXXQfAtGnTWL9+Pffff3+vcD799NO54447uvf59a9/zauvvqoAH2n+jq4A18siIi40hFZwon3lK1/hoosuoqamhsLCQp544gnmz5/P9OnTAaeb/dFHH2Xr1q20trYSCoWIRCKDfvw1a9Ywf/78XiPVTzvttH77Pf/88zz44INs2rSJpqYmIpEIgUCAPXv2DHpAXFNTE9XV1Zxxxhm9tp955pksWLAAv99PRkYGAHPnzu21T3FxMVu2bBn08xoKnQPvwR90zlOoBS4icng+9alP4fP5eOqppwgGgyxatIivfOUrADz77LPcdNNNXHrppbz88sssX76ce+65h2AwOKw1LFmyhC996UucccYZ/PGPf+SDDz7gF7/4BQCBQGBYj9UlJSWl121jzJA+mAyFmpo9tHaoC11EZDh4vV6uuOIKnnzySSZPnkxjYyOXXXYZAG+99Rbz5s3jtttu695/69atQ3r8mTNn8uSTTxIOh/F6nffsd955p9c+b7/9Nj6fj//8z//s3vbcc8/12qcrcHs+Tl85OTmUlpby1ltvceGFF3Zvf/PNNykvL+9ufcebWuA9tAXUhS4iMlyuuuoqPvjgA7797W9z4YUXMnbsWACmT5/OqlWreOGFF6iqqmLBggU8//zzQ3rsG2+8kdraWq677jrWrl3La6+9xl133dVrn+nTp1NbW8uvfvUrNm/ezBNPPMHDDz/ca5/y8nIAFi9eTG1tLS0tLVGP981vfpOf/vSnPPLII2zcuJFf/vKX/PznP+fOO+8cUt3DKa4Bbow5zxiz3hizyRhzR5T7bzPGrDHGrDTGvGaMmRTP+loD6kIXERkuc+bMYe7cuaxYsYKrrrqqe/v111/PlVdeyVe/+lXmzZvHkiVLuPfee4f02CUlJbz44ou8//77zJ07l1tvvZUf/ehHvfa58MILueuuu7jzzjuZPXs2ixYt4gc/+EGvfU466SRuvfVWrr/+egoLC7n55pujHu/GG2/kvvvu43vf+x4zZ87kgQce4Pvf//6IDE4bLGOtjc+BjPECG4BzgWpgKXC5tXZNj33OBpZYa/3GmBuBs6y1l0Z9wE4VFRW2srJyWGr83+U7+bffr+Dv3ziTyQVZw/KYIiKHau3atcyYMSPRZcgIO9jf2RizzFpb0Xd7PFvg84FN1trN1toAsAi4uOcO1trXrbX+zpvvAaVxrK+7BZ6p68BFRMTl4hngJcCOHrerO7fFcg3w52h3GGOuM8ZUGmMqa2trh63ArnPgWo1MRETczpWD2Iwx/wxUAD+Idr+1dqG1tsJaW1FQUDBsx+0ehZ6sABcREXeLZ1/xTmBCj9ulndt6McacA9wFnGmtjT0x7gi49KQJnDo1nySvKz/XiIiIdItngC8FphljynGC+zLgyz13MMbMA34JnGetrYljbQCMy01jXG5avA8rIhKTtTbqcpdyZDicgeRxa2paa0PAzcArwFrgGWvtamPMfcaYizp3+wGQBTxrjFlhjFkcr/pERNzG6/UO++xk4i7BYLDXdLBDEdfh1tbal4GX+2y7p8fP5/T7JRGRo9SYMWPYu3cvJSUleDw6tXekiUQi7N2795DXHtf1UiIiLuXz+aiurmb9+vWJLkVGSGZmJj6f75B+VwEuIuJSHo+HiRMnJroMcSn1yYiIiIxCCnAREZFRSAEuIiIyCinARURERiEFuIiIyCgUt+VER4oxphbYNowP6QPqhvHxRgM956PD0facj7bnC3rOR6pJ1tp+C3+M+gAfbsaYymjrrh7J9JyPDkfbcz7ani/oOR9t1IUuIiIyCinARURERiEFeH8LE11AAug5Hx2Otud8tD1f0HM+qugcuIiIyCikFriIiMgopAAXEREZhRTgIiIio5ACXEREZBRSgIuIiIxCCnAREZFRSAEuIiIyCinARURERqGkRBdwuHw+ny0rK0t0GSIiIiNi2bJlddFWIxv1AV5WVkZlZWWiyxARERkRxpioS2arC11ERGQUUoCLiIiMQgpwERGRUUgBLiIiMgopwEVEREYhBbiIiMgopAAXEREZhRTgIiIio5ACvI+WjlCiSxARETmouAW4MeYxY0yNMeajAfY5yxizwhiz2hjzZrxq6/LgqxuYd99fCUdsvA8tIiIyJPFsgT8OnBfrTmPMGOBh4CJr7SzgS3Gqq1txbjrBsGVnfVu8Dy0iIjIkcQtwa+1bwP4Bdvky8Ly1dnvn/jVxKayH8oJMALbsa433oUVERIbETefAjwHyjDFvGGOWGWOuirWjMeY6Y0ylMaaytrZ22Aoo93UGeG3LsD2miIjISHBTgCcBJwIXAJ8G7jbGHBNtR2vtQmtthbW2oqCg3wprhyw/M4Xs1CS21KkFLiIi7uam5USrgX3W2lag1RjzFnA8sCFeBRhjKC/IZLMCXEREXM5NLfAXgNOMMUnGmAzgZGBtvIso92WyVefARUTE5eLWAjfGPA2cBfiMMdXAt4FkAGvtL6y1a40xfwFWAhHgUWttzEvORkq5L5PFH+6iIxQmNckb78OLiIgMStwC3Fp7+SD2+QHwgziUE1O5LxNrYfs+P9OKshNZioiISExu6kJ3he6R6DoPLiIiLqYA76NMAS4iIqOAAryPnLRkfFkpCnAREXE1BXgU5T5dSiYiIu6mAI+i3JfJVgW4iIi4mAI8inJfFjXNHVpaVEREXEsBHkW5LwNArXAREXEtBXgU5b4sQCPRRUTEvRTgUUzKz8AYBbiIiLiXAjyKtGQvxbnpCnAREXEtBXgMupRMRETcTAEeQ7kvky21LVhrE12KiIhIPwrwGMp9mTS1h6j3BxNdioiISD8K8BgOLGrSkuBKRERE+lOAx3AgwP0JrkRERKQ/BXgMpXnpJHmMWuAiIuJKCvAYkrweJo7N0KVkIiLiSgrwAZT7MtlcqwAXERH3UYAPoNyXybZ9fiIRXUomIiLuogAfQHlBJm3BMHub2xNdioiISC8K8AGU53eORFc3uoiIuIwCfADlBZ0Bvk8BLiIi7qIAH0BRdhrpyV61wEVExHUU4APweAyT8nUpmYiIuI8C/CAmF2QqwEVExHUU4AdR7stk+34/oXAk0aWIiIh0U4AfRLkvi1DEUl3fluhSREREuinAD6LclwGgbnQREXEVBfhBlPuyAAW4iIi4iwL8IPIykslNT1aAi4iIqyjAD8IYQ5lPI9FFRMRdFOCDMFkBLiIiLhO3ADfGPGaMqTHGfHSQ/U4yxoSMMV+MV23drIW2+n6by32Z7Gpsoz0YjntJIiIi0cSzBf44cN5AOxhjvMADwF/jUVA/b/wX/GAqhEO9Nk8tzMJa2FTTkpCyRERE+opbgFtr3wL2H2S3fwX+ANSMfEVR5E6ASAgat/faPHN8DgBrdjUloioREZF+XHMO3BhTAnwe+Pkg9r3OGFNpjKmsra0dviLypzrf91X12jxxbAaZKV7W7FaAi4iIO7gmwIEHgduttQeds9Rau9BaW2GtrSgoKBi+CvKnON/7BLjHYzh2fI4CXEREXCMp0QX0UAEsMsYA+IDPGGNC1tr/jVsFmQWQmgP7NvW7a8b4bF5YsQtrLZ01ioiIJIxrWuDW2nJrbZm1tgx4DviXuIY3gDEwdjLsr+p318zxuTS3hzQnuoiIuELcWuDGmKeBswCfMaYa+DaQDGCt/UW86jio/KlQvbTf5hnjswFYs7uJCWMz4l2ViIhIL3ELcGvt5UPY9+oRLGVg+VNg9fMQ6oCk1O7Nx47LwWNg7e4mPj1rXMLKExERARd1obtG/lSwEajf2mtzeoqXMl+mLiUTERFXUID3NbZrJHr/gWwzx+ewdo8CXEREEk8B3lf+ZOf7vigD2Ypz2LG/jca2YJyLEhER6U0B3ld6HmTkx7iUzJmRbZ2uBxcRkQRTgEczdgrs39xv86zOAF+rABcRkQRTgEeTPzVqC7wgO5X8zBTNyCYiIgmnAI8mfzI074aO3quPGWOYWZzD2t3NCSpMRETEoQCPpmtRkyjd6DPG57B+bzOh8EGnbBcRERkxCvBoui4lizqlag6BUITNda1xLkpEROQABXg0Y7suJYs9El0TuoiISCIpwKNJzYLs8bCvfxf65IJMUpI8GsgmIiIJpQCPJcZI9GSvh+lF2bqUTEREEkoBHkuMZUXBWZlsza4mrLVxLkpERMShAI8lfyr490Fbfb+7Zo7PYV9rgNrmjgQUJiIiogCPLb9rUZPol5IBrFY3uoiIJIgCPJbua8H7d6PPKNaUqiIiklgK8FjyysB4og5ky0lLpjQvXZeSiYhIwijAY0lKhdwJUZcVBec8uC4lExGRRFGADyR/StQWODjnwbfUteIPhOJclIiIiAJ8YF3Lika5XGxmcQ7Wwvo9WthERETiTwE+kPyp0NEErbX97prZvTa4AlxEROJPAT6Q7kvJ+p8HL81LJzs1iTW7G+NclIiIiAJ8YN0B3v88uDGGGVobXEREEkQBPpDcieBJijml6szxOazd3UQkoilVRUQkvhTgA/EmQV55zJHoM8fn4A+E2bpPa4OLiEh8KcAPJn9K1OlUAWaVOAPZPtKELiIiEmcK8IPJn+p0oUci/e46piiblCQPq6obElCYiIgczRTgBzN2MoTaoXlXv7uSvR5mjs9hZbVGoouISHwpwA+ma1GTGOfB55TmsnqXBrKJiEh8KcAPZoBrwQGOK8mlpSPEFg1kExGROFKAH0x2MSSlO1OqRjGnNBeAVepGFxGROFKAH4zH45wHj9GFPrUgi7Rkj86Di4hIXMUtwI0xjxljaowxH8W4/wpjzEpjzCpjzD+MMcfHq7aDyp8Ssws9yethVnEuq3ZqJLqIiMRPPFvgjwPnDXD/FuBMa+1s4LvAwngUNSj5U6B+C4SjLx06uySXj3Y2EdZANhERiZO4Bbi19i1g/wD3/8NaW9958z2gNC6FDUb+VIiEoGFb1LvnlObSFgxTVdsS58JERORo5dZz4NcAf451pzHmOmNMpTGmsra2/1Kfw853jPO9bmPUu2eXaCCbiIjEl+sC3BhzNk6A3x5rH2vtQmtthbW2oqCgYOSL6r4WPHqATy7IIiPFy6qdCnAREYmPpEQX0JMxZg7wKHC+tXZfouvpljEWMnxQtyHq3V6P4bjiXFZqSlUREYkT17TAjTETgeeBK6210ZMykXzTYnahA8zunJEtFO4/Z7qIiMhwi+dlZE8D7wLTjTHVxphrjDE3GGNu6NzlHiAfeNgYs8IYUxmv2gblIAE+pzSXjlCEjTUayCYiIiMvbl3o1trLD3L/14CvxamcofMdA/4nwL/f6VLvo+dAthnjc+JdnYiIHGVc04XuevnTnO8xWuFl+ZlkpyaxUhO6iIhIHCjAB8vXGeAxRqJ7PIZZJTms2tkUx6JERORopQAfrDGTwJsScyQ6wJzSMazd3UQgpIFsIiIyshTgg+VNchY1GWgkekkugVCEDXub41iYiIgcjRTgQzGIkeiAJnQREZERpwAfCt8xnYuaBKPePXFsBjlpSVpaVERERpwCfCjypzmLmtRvjXq3MYY5pWO0tKiIiIw4BfhQdC9qEnsg23Eluazf00xHKBynokRE5GikAB8KX+eiJgOORM8lGLasqrQIrQAAIABJREFU36OBbCIiMnIU4EORlgtZRVC3KeYuXTOy6Ty4iIiMJAX4UPmOGbAFXpqXTl5GstYGFxGREaUAH6r8qU6AWxv1bmMMs0vHsFKXkomIyAhSgA+V7xhobwB/7OXK55TksmFvM+1BDWQTEZGRoQAfqkGMRJ9dmks4Ylm9S61wEREZGYcV4MaYLGPMBcaYacNVkOsNYiR6xaQ8/l979x3fVn0ufvzzSLLlvWdiZ9pJSEISIIQRRhIIBOhml0ILBdpb6KUtLW1vf72X230p0NLeAg2zXMqGstqyk4ZAIAOypzMd2/GK95As6fv74xzbiuPEdiJblvy8Xy+9ztHRkfR8Y+X1nPOdAB/tOjgUESmllBqBBpTAReQpEfl3ez8G+Bh4DdgkIp8ZhPiGn9RCcMUddUrVzCQ3U/NTWL6jZggDU0opNZIM9A58HvCBvf9ZIBnIB+4EfhqyqIYzh9PuyHbkBA4wtyiTNXvraPNqO7hSSqnQG2gCzwAq7f2FwEvGmErgKeCEUAY2rHX2RD+KuUVZeP0BVu3RanSllFKhN9AEXg2Mt/cXAkvs/QRg5CyCnTUJ6veCz3PEU+aMzyDGKXxQotXoSimlQm+gCfx54K8i8g6QArxtH58FHL1OOZpkFYMJwMFdRzwlIdbFyWPSWa4JXCml1CAYaAK/A/g9sBFYaIxptY+PAh4KZWDDWpbd6b6PdvCzirLYXNHIwRbvEASllFJqJBlQAjfG+Iwx9xpjvmOMWRd0/G5jzOLQhzdMZXYm8D7awYuzMAZW7DzypC9KKaXUsRjoMLKZIjIt6PnFIvK8iNwpIq7QhzdMuZMgeVSfd+AzRqeS7HZpNbpSSqmQG2gV+p+BEwFEpAB4AUgCbgJ+EdrQhrmsYqg9egJ3OR2cPjFTO7IppZQKuYEm8MnAp/b+l4BVxpiLgOuAK0MZ2LCXNcm6Az/CoiadzirKYt/BVvbVth71PKWUUmogBprAY4F2e38e8E97fzuQF6KYIkNWMXgaobnyqKfNLcoE4IOdeheulFIqdAaawLcBl4nIGKxx4O/Yx/OBulAGNuz1syf6xOwkclPc2g6ulFIqpAaawP8b+BWwG1hujFltH7+A7qr1kaEfq5KBtT743KIsPiypIRA4enW7Ukop1V8DHUb2CjAGOAW4JOild4EfhDCu4S95FMQk9HkHDlY7eF1rB1sONA5BYEoppUaCAS8naoypNMasBWJFJM4+tsIYsznk0Q1nDoc1J3ofPdHBmhcd0N7oSimlQmbACVxErheREqAZaBaRHSLytZBHFgmyivusQgfITYmjOCeJ5SU6oYtSSqnQGOhELrcB9wOvApfaj9eB+0Xk232891ERqRKRjUd4XUTkDyJSIiLrReTkgcQWFlmToL4UOtr6PHVuURYrd9fi8enyokoppY7fQO/Avw3cZoz5njHmFfvxXeC7wG19vPdxYNFRXr8IKLYfNwMPDDC2oZdVDJh+3YWfVZRFe0eAT/bWD35cSimlot5AE3ghVoe1nt61XzsiY8wy4GiLY38eeMJYPgLSRCR/gPENrdwTre2BDX2eetqEDJwOXV5UKaVUaAw0ge/HmsClp3n2a8djNFDa47tG93aiiNwsIqtFZHV1dfVxfu1xyJwIMYlQsb7PU5PjYphZkKrjwZVSSoXEQBP4A8AfROTX9kImF4vIb4D7sNrGh4QxZrExZrYxZnZ2dvZQfe3hHE7Imw4H+k7gYFWjr99fT0NbxyAHppRSKtoNdBz43Vhrgl+D1XntdeDLwPeNMfccZyxlHFoNX2AfG97yZlhV6IFAn6fOLcoiYOD9HWGsNVBKKRUVjmUc+J+MMWOAVCDVGDPGGBOKDmevAtfZvdFPBxqMMRUh+NzBlT8DvM1Qt7vPU08Zm05+ahzPrz7e1gallFIjXZ9reIvIW3283rVvjLngKOc9jdVWniUi+4H/AmLs9z0I/AO4GCgBWoHr+4x+OMibYW0r1lpt4kfhcjq4fHYhf3xvB6UHWynMSBiCAJVSSkWjPhM4IarGNsZc3cfrBrglFN81pHJOAEeM1ZFt+qV9nn7lqVYCf251KbdfMHkIAlRKKRWN+kzgxpjIuBMOF5cbcqb0uyPb6LR45k3K5rnVpdx2XjEu54BbMZRSSqmBt4GrXuTNtO7ATf9WG7tqzhgqGz0s2aad2ZRSSh0bTeChkD8DWmugqX997hZMySEn2c0zK/cNcmBKKaWilSbwUMifaW0r1vXr9BingytmF7JkWxXl9X3Po66UUkr1pAk8FHKnA9KvGdk6XXlqIQEDz60u7ftkpZRSqgdN4KHgTrKGkPWzIxtAYUYCZxdn8dyqUvyB/rWdK6WUUp00gYdK3owB3YEDXD1nDOUN7Szbrp3ZlFJKDYwm8FDJnwEN+6D1aAuuHer8E3LJSorlae3MppRSaoA0gYdKZ0e2fiwt2inW5eCyUwp5d2sVlY3tgxSYUkqpaKQJPFTyBtYTvdNVpxbiDxie185sSimlBkATeKgkZkLK6AF1ZAMYl5XImRMzeWZVKQHtzKaUUqqfNIGH0jF0ZAOrM9v+ujaWbKsahKCUUkpFI03goZQ/A2p3gLd1QG+7YFou47MS+X8vb6SuxTtIwSmllIommsBDKX8mmABUbhrQ29wuJ3+46iRqmj388MX1mH7Oqa6UUmrk0gQeSsFrgw/QiQWp/HDRFN7aXMmTH+uwMqWUUkenCTyUUgsgPn3AHdk63TB3POdOyubnr29m64HGEAenlFIqmmgCDyWRY+7IBuBwCHdfPpOUuBj+/elPae/whzhApZRS0UITeKjlz4CqzeDvOKa3Zye7ufeKmWyvbOYXf98c4uCUUkpFC03goZY/C/xeqN52zB9xzqRsbj5nAk9+tI83Nh4IYXBKKaWihSbwUOvqyDawGdl6+v4FkzlxdCo/fHE9ZbpmuFJKqR40gYda5kSISTjmjmydYl0O/nD1SfgDhqsWr2B3TUuIAlRKKRUNNIGHmsMJudOPuSNbsPFZiTx542m0ePxc9sCHrN9fH4IAlVJKRQNN4IMhf6a1KlkgcNwfNaswjRe+eQZxMU6uWvwR7+/QtcOVUkppAh8c+TPA2wR1u0PycROyk3jpW2cyJiOBGx5fxStry0LyuUoppSKXJvDBMPoUa7vvo5B9ZG5KHM9+4wxOGpPObc+s5ZHlobk4UEopFZk0gQ+GnKmQmAM73wvpx6bGx/DEDXNYNC2Pn7++me8+u5bSg30vnLJiZy1XPLiCi+57n9++uZVP99Xp0qVKKRXhJNIXzpg9e7ZZvXp1uMM43EvfgJK34fsl4AjtdZI/YLj37W089P5ujDFcc9pYbl1QRFaS+5DzNpc3ctebW1m6rZr81DjGZiawak8d/oAhK8nNeVNyOH9qLmcXZxEX4wxpjEoppUJDRNYYY2YfdlwT+CBZ/xy8dBPcvBRGnTQoX1HR0MYf3t3Bc6v343Y5+PpZ47npnAk0tHZw79vbeXltGSlxMdwyfyLXnTGOuBgnDa0dLN1exdubK/nXtmqaPD5Gp8Xz0HWzmToqZVDiVEopdew0gQ+15iq4uxjO+084+/ZB/apd1c3c8/Z2/r6+gtT4GNq8fkTghrPG881zJ5IaH9Pr+7y+AB+U1PDjlzbQ2N7BvVfMYtH0vEGNVSml1MBoAg+HB8+CuDT42utD8nUbyxp4YOlO0hJiuHVBEfmp8f16X1VjOzf93xrWldZz+8JJ3LqgCBEZ5GiVUkr1x5ESuCscwYwYExfAivvB0wzupEH/uumjU/nTNScP+H05KXE8e/Pp/PilDdzz9na2VzVz16UziI/tbhevaGjjva1VvLelCgP8z6UzyE52H/lDlVJKDSpN4INp4gL44D7YsxwmLwp3NEcVF+Pk3itmMik3mbve3MqemhZ+cOFkVu4+yLtbq9hSYa1PXpAeT22zl6sf+oinbjqNnOS4MEeulFIj05AOIxORRSKyTURKRORHvbw+RkSWiMinIrJeRC4eyvhCbswZ4IoP+XCywSIi/Nu8iTx07Wx2VTdz3aMruX9pCcluFz+6aApvf/cc3r9jPo9dfyrl9W1ctfgjKhvbwx22UkqNSEPWBi4iTmA7sBDYD6wCrjbGbA46ZzHwqTHmARGZCvzDGDPuaJ87rNvAAZ68DOr2wLeHcYy92FvbwpaKRk6fkElaQuxhr6/cfZDrH1tJTkocT990OnmpeieulFKD4Uht4EN5Bz4HKDHG7DLGeIFngM/3OMcAnWOZUoHyIYxvcExcALU7oH5fuCMZkLGZiSyant9r8gaYMz6DJ74+h+omD1cuXkH5EZY87fAHqG32DGaoSik1Ig1lG/hooDTo+X7gtB7n3Am8JSLfBhKB83v7IBG5GbgZYMyYMSEPNKQmLrC2O5fAKV8NbywhdspYK4l/9ZGVXLl4BU/deDpef4D1++tZV9rAuv31bC5vxOMLcNH0PL67cBKTcpPDHbZSSkWFoaxCvwxYZIy50X5+LXCaMebWoHO+Z8d0j4icATwCTDfGHHFZr2FfhW4M/G4aFMyGK54IdzSDYl1pPdc+8jGN7b6uY/ExTqaPTmFmQRoup4MnP9pLi9fHZ2eM4rbzi5mYPfi98pVSKhoMh2FkZUBh0PMC+1iwrwOLAIwxK0QkDsgCqoYkwsEgAhPnw5bXIOC31guPMjML03j2G2fwt0/LKMpOYkZhKkXZSbic3S003zhnAovf38XjH+zh9fXlfOGk0dx2XjFjMxPDGLlSSkWuobwDd2F1YjsPK3GvAr5sjNkUdM4/gWeNMY+LyAnAu8Boc5Qgh/0dOMDGF+GFG+DGd6078RGsptnDn/+1kydW7MUfMFx5aiG3nVdMTop2glNKqd6EvRObMcYH3Aq8CWwBnjPGbBKRn4nI5+zTbgduEpF1wNPA146WvCPG+HmARMxwssGUleTmJ5dM5f075vPl08bw7KpSzv3tUn775lYa2jrCHZ5SSkUMnUp1qCyeB644uOGNcEcyrOytbeGet7bz6rpy0hJi+Na87oVXlFJK6Vzo4ffuz2H57+CHuyEuNdzRDDsbyxq4681tLNteTW6KmzMmZDI5L4XJeUlMzkthVGqczs+ulBqRhkMntpFt4gJ4/27Y/T6c8JlwRzPsTB+dyhM3zOHDnTU8unw3K3cf5OW13dMAJLtdTMlP5uZzJrJwam4YI1VKqeFBE/hQKTgVYpOsdnBN4Ed05sQszpyYBUBDWwc7KpvYeqCJ7ZVNLC+p4aYnVvOFWaP4r89OIz2x90lmlFJqJNAEPlRcsTDubO3INgCp8THMHpfB7HEZgLV++f1LS/jf90pYXlLLL784nQun6frlSqmRaUgXMxnxJi6Aut1wcFe4I4lIsS4H3zl/Eq/cOpecZDff+L81/PvTn3Kwxduv93f4A1Q1trPtQBNtXv8gR6uUUoNL78CHUvH58E9g09/g7NvDHU3EmjYqlVduncsDS3fyx/d28OHOGmYWpB12ngEa2zo42OKltsV7yDC1/NQ4fvXFE5k/JWcII1dKqdDRXuhD7S+fhYO74bZ1UTkr21DbUtHIb/65ldqW3hdMSXbHkJEUS1ZiLBmJbjKSYkmIcfLgv3ayo6qZL8waxX9+dhoZ2p6ulBqmdBjZcLH5VXjuWrjqKZhySbijGbE8Pj9/WrKT+5eUkBofw52fm8ZnZuT3e6iaMQZ/wOB0iA5vU0oNKk3gw4XfB/fNgKxJcN3L4Y5mxNt6oJE7XljP+v0NnH9CLl84aRQHGtopr2+nvL6NioY2yhvaaWrvIBAAv524O41KjePf5k3kilMLcbu0RkUpFXqawIeTf/0WlvwCbl0DWUXhjmbE8/kDPPbBHu5+axsen7XwXUKsk1Fp8eSnxjE6LZ7U+BgcDsEpgsMhOAQcIizbXs3qvXXkpViJ/MpTC484i1znXXvwIi9KKdUXTeDDSXMV3DsVTr0RLvpNuKNRtqqmdmqavIxOiycl3tWvqnFjDB/urOW+d3awcs9BclPcfPPciVxyYj67a1rYXtnEtsomth9oZltlEy0eH5PzkplZmMasgjRmFqZRlJOE06HV8Eqp3mkCH25e+DrseBtu3wKxuqRmpDPGsGJXLb9/Zwcrdx885LXkOBeTc5OZnJdMclwMm8obWFtaT5O9fnpCrJNTxqbzjXMmMrcoU9vUlVKH0KlUh5s5N8HGF2D9czD7+nBHo46TiHTNIvfRrlo2ljVQlJPE5Lxk8lIOn8c9EDDsqW1h3f561pU28NamA3zlkY+ZMz6D2xdO4rQJmWEqiVIqUugdeLgYAw+eDRj45nLQu64RzePz88zKUv60pISqJg9nF2fxvYWTOGlMerhDU0qFmVahD0drHofXboPr34CxZ4Q7GjUMtHf4efKjvdy/dCcHW7ycXZzFnHEZFOUkUZSTxNjMRGJdh3aC6/AHqGxs50BDOzXNXopykpiYnahV8UpFCU3gw5G3Be45wZqh7bJHwx2NGkZaPD4e/3APz6zaR+nBtq7jLocwJjOBwvQE6lu9VDS0U93soed/48zEWGaPS+fUcRmcOi6DqaNSiNHe70pFJG0DH45iE+Gka2DlQ9BUCcm6TKayJLpd3DK/iFvmF9Hq9bGruoWSqmZ2VDVRUtXM/ro2MhJjmZyXTH6qNdwtPy2etPgYtlQ0smpPHav2HOTNTZUAxMc4KUiPJy81jpzkOHJT3F37RTlJjM9K7FdP+A5/AFc/J68JBAxvbT5AdZOH+VNyKEhPOOZ/D2MMTR4fKXExx/wZSkUbvQMPt9qd8MeTYf5P4Nw7wh2NijKVje2s3lPHJ/vqKKtro7KpncqGdqqaPPiCJqSJi3EwOS+FqfkpTB2Vwgl5yXh8AXbVtLCrupndNS3srmmh9GArYzIS+PrZE7js5ALiYw8f826M4V/bq/ntm9vYVN7YdXzaqBQumJrHBdNymZKX3O8q/roWL995di0f7qzhtvOK+ea5E3UsvRpRtAp9OPu/L0LVVvjOBnBqpYgafIGAobbFy4GGdrZXNrG5opHN5Y1srmg8ZNEXsO7ex2clMiE7kTEZCXyws5Z1pfWkJ8Rw7eljufaMcWQnuwFYs7eOu97Yyse7D1KQHs/3Fk5iZmEa726p5K1NlazZV4cxUJAez+dmjuLGsyccdR76daX1fOuvn1Dd5OHU8el8UFLLzIJU7rliFkU5SYP6b6TUcKEJfDjb+g945mr40sMw4/JwR6NGMGMM5Q3tbK1oJD7GyYTsJHJT3IfcLRtjWL23jsXLdvHOlkpinA6+dNJoapq9vLOlkqykWL69oJir54w5rMNddZOHd7dU8uamAyzdXk1irIsbzx7PjWdPIMntOuQ7nvx4Hz9/bTPZyW4e+MrJzChI47V15fz0lY20ef384MLJ3DB3PA6dBEdFOU3gw1nADw/Nh8ZyuGUlJGSEOyKl+mVXdTOPLN/NC2v2E+t08I1zJ3D93PEkuvuuSdpe2cS9b23njU0HyEiM5VvzJvKV08cSMIaf/G0jf/u0jHmTs/ndFbNID7pLr2pq5z9e2sA7W6qYMz6Duy+byZjMY29fV2q40wQ+3B3YCIvPhWlfhEsfDnc0Sg1IU3sHDpF+Je6e1pXWc/db23h/Rw15KXEkup3sqmnhu+dP4tb5Rb3eYRtjePGTMv771U00e31kJsaSnRxHTrKb7GQ3OcluMhJj8QcMXl8Arz+AxxfAa891f9r4DM6dnE1C7JHjbfP6WbKtinc2V9LQ1tG1kE2ga2s1BZw4OpUTR6cydVTKUT9PqWOlCTwSLP0NLP01XPU0TLk43NEoNaRW7Kzl7re2saemhd9fNYuzi7P7fE95fRvPrS6lsrGdqkYPVU0eqps8VDd7Dlk1ziEQ63IQ63TgDxhavH7iYhycOymbRdPzWDAll9T4GNo7/CzdVs3fN1Tw7pZKWr1+MhNjyU+LwylW73unvagNwK6aFmqaPV3fUZSTxPTRqVxyYj4LpuToWHwVEprAI4HPa1Wlt1TDtz7SqnQ1InWus348AgFDU7sPl1NwuxyH9Fr3+QOs2lPHGxsreGPTASobPcQ4hZkFaWypaKTF6ycjMZZF0/P4zIx8ThufecR4jDFUNnrYUNbAhrIGNpY1sK60ntoWL7MK07jjwsmcWZQ14Phrmj08s3IfZfXtzCpM5eQx6UzMTtL2/hFKE3ikqFgHi+fDjCvgiw+GOxqlologYFi7v543Nx7gg501TB+VyiUz8jljQuYxD1Xr8Ad4cc1+7nt3BxUN7cwtyuT7F0zu17S4G8saeOyDPby2rhyvP0BynKtr0ZvkOBezCtM4eUw6hRkJ1DZ7qGm2ahxqmr1UN3kIGMPscemcPiGT0ydkkpsSd0xlUMOLJvBI8t4vYNlv4cvPwaQLwx2NUuoYtHf4+evH+7h/SQm1LV7OPyGXz80aRbLbRaLbRZL9SHA7Wbn7II99sJtVe+pIiHVy6ckFfPXMcUzISmR3bQuf7K3j09J6Ptlbx/bKJjpbB+JiHGQnu8lKcpOd5MYXMKzac7Ar6U/ISuS0CZmcVZTF+VNzcLt6X6teDW+awCOJzwN/PhfaG+BbKyA+LdwRKaWOUYvHx2Mf7ObPy3Z1JdbeFKTH87Uzx3H57EJS448841yzx0dNk4esZDeJsc7D2tn9AcPm8kY+2lXLR7tqWbn7IE0eH1lJbq45bQzXnD6GnGS9M48kmsAjTdkn8PD5MOtq+Pyfwh2NUuo4tXh8lNe30ezx0eLx0+zpoNnjp7m9g4L0BOZPyTnutv/e+AOG5SU1PP7BbpZsqybGKXx2xii+NnccMwr05iASaAKPRO/cCct/p73SlVIhsau6mSdW7OX51aW0eP3MKEhlUm4yo+y59PNT4xiVFs+otPhDJtbpS12Ll+Q413FNcVvZ2M5r68pJjnOxaHr+UWshRhpN4JGoox0eWQg1O+ArL8K4ueGOSCkVBRrbO3hh9X5eX19OWX0bVU2Hr2h34uhUzpmUxTnF2Zw8Nv2Q1ex8/gBr9tbx3rYqlm6tZltlE26Xgyn5KUwflcK0UalMH53CpNxk4mKO3O7u8fl5Z3MVz68pZdn26q62/Ving/NOyOELJ41m3uTsEd92rwk8UjVXw+MXQ2MFXPcKFJwS7oiUUlGmwx+gqslDRX0b5Q3t7K5uYXlJNZ/sq8cfMCS5XZw5MZNTxqazoayBZduraWz34XIIc8ZncFZxFnUtXjaWNbKxvKGrrd/lEPLT4shPibe29sp52cluPtpVyytry2lo6yA/NY5LTy7g0lMKaGrv4G+flvHaunJqmr2kxsdwyYx8Fp6Qy/TRqV3z7o8kmsAjWWM5PLrI6tT2tb9D3vRwR6SUGgEa2zv4sKSGf22vYdn2asrq28hKcjN/cjYLpuRwVnEWyT2WeDXGUHqwjU3lDWwqb6S0rpWKhnYqGto40NBOh9/KOW6Xgwun5XH57ALOnJh1WPu/zx9geUkNL39axpubKmnr8AOQm+Jm+qhUpo+2HjMKUgdluJwxhrL6NnZVtxAwBhGhM0IRiHE6mJCdSHaSe9An7BkWCVxEFgH3AU7gYWPMb3o55wrgTsAA64wxXz7aZ46IBA5QtwcevQgCHXD9G5BVFO6IlFIjiDGG6mYPWYnuY55QpnMVvMrGdgozEvrdzt3q9bFhfwMbyxvZaE+Ys7O6uavKPS8ljlmFacwsTGNWYRonFqR2teF7fH6a2n32owOvL0Csy4Hb5cTtcuCOsfbbOvxsLGtgw/4G1tvfcbDF22dsGYmxTM5NZnJeMlPyOrcpvS61e6zCnsBFxAlsBxYC+4FVwNXGmM1B5xQDzwELjDF1IpJjjKk62ueOmAQOUL0dHrsIXG64/p+QPjbcESmlVFi0en1sqWhk/f4G1pbWs7a0nr21rYA1rW16QixNHl/X/Pf95XQIk3KTOXF0CicWpDEpJ8nunGflSmOsvTavn5KqZrYdaGJrZRPbDzR11RIsvvYULpiWF7KyDocEfgZwpzHmQvv5jwGMMb8OOucuYLsxpt+reYyoBA5wYAM8fgnEZ8D1/4CUUeGOSCmlhoW6Fi9r99ezdl89VU0eUuJcJMe5SI6L6dq6XQ68PmtxG4/P37XvdAhTR6UwNT/lqB3vjiQQMOw72MrWA03MGZ9x1HXuB2o4JPDLgEXGmBvt59cCpxljbg0652Wsu/S5WNXsdxpj3ujls24GbgYYM2bMKXv37h2CEgwj+1fDE58HdzJc/hcYc1q4I1JKKTVIjpTAj33Q3uBwAcXAPOBq4CEROWymAWPMYmPMbGPM7OzsvlcsijoFs+GGN8AVZ/VQ//jPHDYGRCmlVFQbygReBhQGPS+wjwXbD7xqjOkwxuzGuhsvHqL4IkveiXDzUihaCP+8A168Ebwt4Y5KKaXUEBnKBL4KKBaR8SISC1wFvNrjnJex7r4RkSxgErBrCGOMLPFpcNVTsOCnsOkleOg8qCkJd1RKKaWGwJAlcGOMD7gVeBPYAjxnjNkkIj8Tkc/Zp70J1IrIZmAJ8ANjTO1QxRiRHA445/vwlZegpQoWz4PNr4Q7KqWUUoNMJ3KJJvWl8PxXoWwNnPxVWPRriE0Md1RKKaWOQ6R0YlPHI63QmuTlrO/CJ09Yd+MV68MdlVJKqUGgCTzauGLh/DvhupehvREePg9W3A+BgU1moJRSanjTBB6tJsyDf/sQis6HN38MT10OTZXhjkoppVSIaAKPZomZVi/1i++GPcvhjyfDuz+D1oPhjkwppdRx0gQe7URgzk3wzeVQvBDevwd+PwPe+yW01YU7OqWUUsdIE/hIkVUMlz9uVatPnA/L7oLfz4Slv4G2+nBHp5RSaoB0GNlIdWCDlby3vg5Ot5XUT/gsTL4YEjLCHZ1SSinbkYaRucIRjBoG8k6Eq/5qDTNb9wxseQ22vwHihHFz4YTPQfEFkDbGqoZXSik1rOgduLKDDOVBAAAMBElEQVQYAxVrrUS++VWo3WEdj0+HvBmQPwPyZ1n7mRPBEbrF6pVSSh1Z2JcTHSyawAdJ9TbYvQwOrLfu0qs2g99rveZ0W3fm6WMhbSykj7P2UwshMQsSMof/DHCBgFUevwd8nVuP9Zo4rAsUcXZvnTEQk2BttUZCKTWEtApdDUz2ZOvRyd9hJfUD66FqC9Tvhbo91trk7b10gnPFW4k8IaN7G99jPz4dXG5wxlqJ0RlrP1wQ8NsJ1mt9d+e+zwO+dmvb0db9vKMNfG3Q0Q4drd3HvC3gbba2nmZ7v7n7YmSgxGGVLSbO2nbF77K2jhirLA5Xd/LvuiBw2A8BpHsLh+4fwlj/FgFf0NZ+mIBVc2IC3Q86L8jtzxeHve+wYo1NgNgk62IkNgFiEq115eNSwJ1ib1OtbeffyKF9XZUajjSBq/5xxkDedOvRU1u9ldAbyqC1BlprrUdLbfd+/V5r/HlvyT5kMbqtxBqTYK2VHhNv1QTEJkJSrpW43EnWc1e8NWud092dhF1uQMD4rWTZtQ3YFwr2BULnxUFHm3Xn7u+wHoHOCw2fdU7ne43fuuM3/u6kiwlaw90cfT13hyvo4TzCxUFMj4sAc+j3mIA1bLCxDLyt0NFibX1tR/83Fad10ZWYbdWuJGZBQlaPi7PMQy/MYuKO9y+plOoHTeDq+MWnWY/8mX2fG/BbCb/toLX1e+w7a++hd9zOmKC78pjuu1uX20rMLreVpF1x3fvaLj9wAT94msDTaG3bG6399kb7IqzauihrqbH2yz6xLsQ8DUf+zNgku7YlI6jmpec2vfsOPz7duvvXO32lBkQTuBpaDqc1Q1xiZrgjUWD9PTovwAbC32Hd0XfVttRYF2WttdBqH+98fnC3td9+lKQvDohLsxN6mlWt7062qvO79pOti7eu6v+E7uedNSgu96G1Ks4Y68JPLw5UFNIErpQaOGcMJOVYj/7y+6wmlNaDdnK3m1Ta6no86q3agKbK7poBTxPd7fvHQoL6JsRYFy7BfRUc9n5ws0rwxUBnU0xsUo9tQlCTTdAFRUx8977LrR0f1aDQBK6UGhpOV3c7+kAFAlbnQ5/dSdHbavdDaO3ui+DzdHd07NwGOqwLh4DP3u84tCNgwHf46752q0mnvbH7c72t3R0gA74BBi/dST02AWKTreTvTrIvBOyLgd5qFzr7cvRsMnK6u5uWXEH7DhddnRZF9MIhymkCV0oNfw6H1TOelPDGYYx1ceBtsWoFOi8iOi8sOjs3dj2CLzRaD70QaG+ExoruURKdIykGQ9cIiKChkY7O5z07SdrHDjknaFhl579DZ41IcAfMrguHztEPcvjnd9V8uLo/M7iDZm/DOB0uuxlEgsoSNKojeCRG1+MINTbG9BjN4e8e4RH8/oD/0FEemMM//7BY7edzbobcaYPztwyiCVwppfpLpLt6fTCmHA4E7NEObd1JvbMWoHP4ZOe2t2GWfh/diabHKISuERH+oCQVPDyxl2GKh4zGsLddd/Vy6H7wd0H3+32eXmo9OoISZ/D39vJ9x9V0chTiOPTi4rDhnvbFiwQNyQx+BJcxeKRJwA/Tvjg4MfegCVwppYYLh6O7vf1YmhqiUecdc8+73+DnhyVe+269N521AlHQvKAJXCml1PAldgdEdRgdW6GUUkpFIE3gSimlVATSBK6UUkpFIE3gSimlVATSBK6UUkpFIE3gSimlVATSBK6UUkpFIE3gSimlVATSBK6UUkpFIDFHmvA9QohINbA3hB+ZBdSE8PMigZZ5ZBhpZR5p5QUtc7Qaa4zJ7nkw4hN4qInIamPM7HDHMZS0zCPDSCvzSCsvaJlHGq1CV0oppSKQJnCllFIqAmkCP9zicAcQBlrmkWGklXmklRe0zCOKtoErpZRSEUjvwJVSSqkIpAlcKaWUikCawIOIyCIR2SYiJSLyo3DHMxhE5FERqRKRjUHHMkTkbRHZYW/TwxljKIlIoYgsEZHNIrJJRG6zj0dzmeNEZKWIrLPL/N/28fEi8rH9+35WRGLDHWuoiYhTRD4Vkdft51FdZhHZIyIbRGStiKy2j0XzbztNRF4Qka0iskVEzojm8vZFE7hNRJzAn4CLgKnA1SIyNbxRDYrHgUU9jv0IeNcYUwy8az+PFj7gdmPMVOB04Bb77xrNZfYAC4wxM4FZwCIROR34H+B3xpgioA74ehhjHCy3AVuCno+EMs83xswKGgsdzb/t+4A3jDFTgJlYf+toLu9RaQLvNgcoMcbsMsZ4gWeAz4c5ppAzxiwDDvY4/HngL/b+X4AvDGlQg8gYU2GM+cTeb8L6Dz+a6C6zMcY0209j7IcBFgAv2MejqswAIlIAXAI8bD8XorzMRxCVv20RSQXOAR4BMMZ4jTH1RGl5+0MTeLfRQGnQ8/32sZEg1xhTYe8fAHLDGcxgEZFxwEnAx0R5me2q5LVAFfA2sBOoN8b47FOi8ff9e+AOIGA/zyT6y2yAt0RkjYjcbB+L1t/2eKAaeMxuJnlYRBKJ3vL2SRO4OoSxxhVG3dhCEUkCXgS+Y4xpDH4tGstsjPEbY2YBBVi1S1PCHNKgEpHPAFXGmDXhjmWInWWMORmr6e8WETkn+MUo+227gJOBB4wxJwEt9Kguj7Ly9kkTeLcyoDDoeYF9bCSoFJF8AHtbFeZ4QkpEYrCS91+NMS/Zh6O6zJ3sKsYlwBlAmoi47Jei7fc9F/iciOzBav5agNVeGs1lxhhTZm+rgL9hXaxF6297P7DfGPOx/fwFrIQereXtkybwbquAYrvXaixwFfBqmGMaKq8CX7X3vwq8EsZYQspuB30E2GKMuTfopWguc7aIpNn78cBCrLb/JcBl9mlRVWZjzI+NMQXGmHFY/3ffM8ZcQxSXWUQSRSS5cx+4ANhIlP62jTEHgFIRmWwfOg/YTJSWtz90JrYgInIxVjuaE3jUGPPLMIcUciLyNDAPawm+SuC/gJeB54AxWEuzXmGM6dnRLSKJyFnA+8AGuttG/wOrHTxayzwDqzOPE+si/TljzM9EZALW3WkG8CnwFWOMJ3yRDg4RmQd83xjzmWgus122v9lPXcBTxphfikgm0fvbnoXVSTEW2AVcj/0bJwrL2xdN4EoppVQE0ip0pZRSKgJpAldKKaUikCZwpZRSKgJpAldKKaUikCZwpZRSKgJpAldKDToRmScixp6vXCkVAprAlVJKqQikCVwppZSKQJrAlRoBROTbIrJVRNpFZIeI/KRzjnAR2SMiv7RXd2oUkRoR+ZWIOILenywifxaRahHxiMhqEbmgx3fkiMhjIlJpf882EbmhRygniMgyEWkVkc0ictEQFF+pqOTq+xSlVCQTkTuxppz8DrAWOAF4EIgDfmqf9m2saYRPxVoQ40GsqXbvs19/1H7tK8A+4JvA6yIywxiz1Z5z/V9AG3AN1jSXRVhTmAa7G/gh1vKm/wE8KyJjjTF1oS21UtFPp1JVKoqJSAJQA3zJGPNG0PHrgD8YY9LsFbxKjTFnB73+K+BaY0yhiBQBO4BLjDH/CDrnE2CtMeYGEfk68CegyBizv5c45mEtLHJp54pwIpKLtX7zImPMm6Euu1LRTu/AlYpu04B44EURCb5adwJxIpJtP1/R430fAD8WkRRgqn1sWY9zlmEtUwpwCrC5t+Tdw9rOHWNMpYj4gdx+lUQpdQhN4EpFt8527MuB7b28PtSrNnl7OaZ9cZQ6BvofR6notgloByYYY0p6efjt807v8b4zgTJjTKP9GQDn9DjnHKz1pwHWAFN1nLdSQ0cTuFJRzBjTDPwK+JWI3CIik0VkmohcJSL/E3TqLBG5U0QmiciXgduAe+zP2Ak8D9wvIheKyBQRuQ+YDvzWfv/TWGsxvyoi54vIeBE5T0SuHKqyKjXSaBW6UlHOGPNzEakAbsVKym1Y1emPB532R2AssBroAP6X7h7oADdiJesngRRgA/AZY8xW+ztaReRc4C7gGSAJ2AP8ZrDKpdRIp73QlRrh7F7oDxtjfhHuWJRS/adV6EoppVQE0gSulFJKRSCtQldKKaUikN6BK6WUUhFIE7hSSikVgTSBK6WUUhFIE7hSSikVgTSBK6WUUhHo/wNMBi6iQi5ElQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 504x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RVSRoZop-po",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "86b9b748-1eaf-4c4f-ac09-107dc44b1f5a"
      },
      "source": [
        "# We evaluate our best model on our test set\n",
        "test_metrics = model.evaluate(test_gen)\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4855 - acc: 0.8314\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.4855\n",
            "\tacc: 0.8314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl4KWiplQn8g",
        "colab_type": "text"
      },
      "source": [
        "### We get a test accuracy of **83.14% !** Which is better than our previous graphsage model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKMkK_tqpwfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We make predictions on a new dataset\n",
        "all_nodes = data_dic[\"test\"].index\n",
        "all_mapper = generator.flow(all_nodes)\n",
        "all_predictions = model.predict(all_mapper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv4HnYDQqETi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d022a0f-be42-457a-af82-c004c1fc8a74"
      },
      "source": [
        "# As these data are under the shape of the softmax output layers, we need to inverse_transform function to get them under the shape of original categories.\n",
        "node_predictions = target_encoding.inverse_transform(all_predictions.squeeze())\n",
        "node_predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 0, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXusEUOdqI9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We download the new predictions under a csv format\n",
        "from google.colab import files\n",
        "\n",
        "submit = pd.DataFrame(data=node_predictions , index = data_dic[\"test\"].index, columns = [\"label\"])\n",
        "submit.to_csv('GAT_4.csv') \n",
        "files.download('GAT_4.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhy811Wm2gBN",
        "colab_type": "text"
      },
      "source": [
        "# Graph Convolutional Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kodlyRW22hGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from stellargraph.layer import GCN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjNjqp6R2kka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "409346fa-36f6-4893-a844-88f7b3358184"
      },
      "source": [
        "# We use the FullBatchNodeGenerator function using the gcn method to put our data under the correct shape for a GCN model.\n",
        "generator = FullBatchNodeGenerator(G, method=\"gcn\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVxCrHNS2mK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create an object forh the generator.flow function with the set of nodes and the true labels that will be used to train the model.\n",
        "train_gen = generator.flow(train_subjects.index, train_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arsstQj_2o0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create a 2 layers GCN with 16 unites per layer.\n",
        "# We will use the relu activation function for the 2 layers.\n",
        "# We use a dropout of 0.5\n",
        "gcn = GCN(\n",
        "    layer_sizes=[16, 16], activations=[\"relu\", \"relu\"], generator=generator, dropout=0.5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zixOaf-K2r-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create the input and output tensors that our model will use for its predictions.\n",
        "x_inp, x_out = gcn.in_out_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLJMe-5Q2uxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The output layer will be a softmax \n",
        "predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vux814GI2xNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We create the keras model\n",
        "model = Model(inputs=x_inp, outputs=predictions)\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adamax(lr=0.01),\n",
        "    loss=losses.categorical_crossentropy,\n",
        "    metrics=[\"acc\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AIQ2C4t22IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use test_gen to calculate our validation loss anc accuracy\n",
        "test_gen = generator.flow(test_subjects.index, test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3TYUvNy25UM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use a callback if validation accuracy stops improving\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es_callback = EarlyStopping(monitor=\"val_acc\", patience=20, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TjZxQMy27tj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34c23b01-7a49-45b7-f86a-5fa041c9a3df"
      },
      "source": [
        "# We train our model\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=300,\n",
        "    validation_data=test_gen,\n",
        "    verbose=2,\n",
        "    shuffle=False, \n",
        "    callbacks=[es_callback],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 - 1s - loss: 1.6084 - acc: 0.1711 - val_loss: 1.5891 - val_acc: 0.4797\n",
            "Epoch 2/300\n",
            "1/1 - 1s - loss: 1.5869 - acc: 0.4306 - val_loss: 1.5622 - val_acc: 0.5305\n",
            "Epoch 3/300\n",
            "1/1 - 1s - loss: 1.5612 - acc: 0.4699 - val_loss: 1.5292 - val_acc: 0.5430\n",
            "Epoch 4/300\n",
            "1/1 - 1s - loss: 1.5285 - acc: 0.4825 - val_loss: 1.4939 - val_acc: 0.5567\n",
            "Epoch 5/300\n",
            "1/1 - 1s - loss: 1.4963 - acc: 0.4940 - val_loss: 1.4573 - val_acc: 0.5634\n",
            "Epoch 6/300\n",
            "1/1 - 1s - loss: 1.4625 - acc: 0.5005 - val_loss: 1.4199 - val_acc: 0.5736\n",
            "Epoch 7/300\n",
            "1/1 - 1s - loss: 1.4266 - acc: 0.5180 - val_loss: 1.3811 - val_acc: 0.5959\n",
            "Epoch 8/300\n",
            "1/1 - 1s - loss: 1.3930 - acc: 0.5325 - val_loss: 1.3408 - val_acc: 0.6205\n",
            "Epoch 9/300\n",
            "1/1 - 1s - loss: 1.3537 - acc: 0.5526 - val_loss: 1.2997 - val_acc: 0.6498\n",
            "Epoch 10/300\n",
            "1/1 - 1s - loss: 1.3186 - acc: 0.5773 - val_loss: 1.2580 - val_acc: 0.6733\n",
            "Epoch 11/300\n",
            "1/1 - 1s - loss: 1.2828 - acc: 0.6070 - val_loss: 1.2157 - val_acc: 0.6941\n",
            "Epoch 12/300\n",
            "1/1 - 1s - loss: 1.2434 - acc: 0.6211 - val_loss: 1.1729 - val_acc: 0.7070\n",
            "Epoch 13/300\n",
            "1/1 - 1s - loss: 1.2039 - acc: 0.6339 - val_loss: 1.1294 - val_acc: 0.7222\n",
            "Epoch 14/300\n",
            "1/1 - 1s - loss: 1.1660 - acc: 0.6547 - val_loss: 1.0855 - val_acc: 0.7347\n",
            "Epoch 15/300\n",
            "1/1 - 1s - loss: 1.1294 - acc: 0.6623 - val_loss: 1.0414 - val_acc: 0.7508\n",
            "Epoch 16/300\n",
            "1/1 - 1s - loss: 1.0936 - acc: 0.6743 - val_loss: 0.9974 - val_acc: 0.7613\n",
            "Epoch 17/300\n",
            "1/1 - 1s - loss: 1.0582 - acc: 0.6921 - val_loss: 0.9542 - val_acc: 0.7696\n",
            "Epoch 18/300\n",
            "1/1 - 1s - loss: 1.0185 - acc: 0.6998 - val_loss: 0.9120 - val_acc: 0.7801\n",
            "Epoch 19/300\n",
            "1/1 - 1s - loss: 0.9759 - acc: 0.7163 - val_loss: 0.8713 - val_acc: 0.7876\n",
            "Epoch 20/300\n",
            "1/1 - 1s - loss: 0.9444 - acc: 0.7212 - val_loss: 0.8322 - val_acc: 0.7926\n",
            "Epoch 21/300\n",
            "1/1 - 1s - loss: 0.9236 - acc: 0.7292 - val_loss: 0.7954 - val_acc: 0.8032\n",
            "Epoch 22/300\n",
            "1/1 - 1s - loss: 0.8834 - acc: 0.7361 - val_loss: 0.7610 - val_acc: 0.8067\n",
            "Epoch 23/300\n",
            "1/1 - 1s - loss: 0.8565 - acc: 0.7389 - val_loss: 0.7291 - val_acc: 0.8114\n",
            "Epoch 24/300\n",
            "1/1 - 1s - loss: 0.8260 - acc: 0.7477 - val_loss: 0.7001 - val_acc: 0.8146\n",
            "Epoch 25/300\n",
            "1/1 - 1s - loss: 0.8143 - acc: 0.7527 - val_loss: 0.6738 - val_acc: 0.8149\n",
            "Epoch 26/300\n",
            "1/1 - 1s - loss: 0.7854 - acc: 0.7571 - val_loss: 0.6500 - val_acc: 0.8192\n",
            "Epoch 27/300\n",
            "1/1 - 1s - loss: 0.7631 - acc: 0.7648 - val_loss: 0.6287 - val_acc: 0.8196\n",
            "Epoch 28/300\n",
            "1/1 - 1s - loss: 0.7513 - acc: 0.7592 - val_loss: 0.6098 - val_acc: 0.8192\n",
            "Epoch 29/300\n",
            "1/1 - 1s - loss: 0.7330 - acc: 0.7654 - val_loss: 0.5930 - val_acc: 0.8208\n",
            "Epoch 30/300\n",
            "1/1 - 1s - loss: 0.7130 - acc: 0.7725 - val_loss: 0.5782 - val_acc: 0.8220\n",
            "Epoch 31/300\n",
            "1/1 - 1s - loss: 0.7037 - acc: 0.7667 - val_loss: 0.5655 - val_acc: 0.8228\n",
            "Epoch 32/300\n",
            "1/1 - 1s - loss: 0.6889 - acc: 0.7721 - val_loss: 0.5547 - val_acc: 0.8224\n",
            "Epoch 33/300\n",
            "1/1 - 1s - loss: 0.6804 - acc: 0.7710 - val_loss: 0.5457 - val_acc: 0.8236\n",
            "Epoch 34/300\n",
            "1/1 - 1s - loss: 0.6750 - acc: 0.7740 - val_loss: 0.5383 - val_acc: 0.8243\n",
            "Epoch 35/300\n",
            "1/1 - 1s - loss: 0.6732 - acc: 0.7747 - val_loss: 0.5320 - val_acc: 0.8247\n",
            "Epoch 36/300\n",
            "1/1 - 1s - loss: 0.6575 - acc: 0.7750 - val_loss: 0.5267 - val_acc: 0.8259\n",
            "Epoch 37/300\n",
            "1/1 - 1s - loss: 0.6561 - acc: 0.7737 - val_loss: 0.5221 - val_acc: 0.8259\n",
            "Epoch 38/300\n",
            "1/1 - 1s - loss: 0.6437 - acc: 0.7818 - val_loss: 0.5182 - val_acc: 0.8267\n",
            "Epoch 39/300\n",
            "1/1 - 1s - loss: 0.6391 - acc: 0.7783 - val_loss: 0.5149 - val_acc: 0.8271\n",
            "Epoch 40/300\n",
            "1/1 - 1s - loss: 0.6444 - acc: 0.7825 - val_loss: 0.5120 - val_acc: 0.8263\n",
            "Epoch 41/300\n",
            "1/1 - 1s - loss: 0.6264 - acc: 0.7903 - val_loss: 0.5097 - val_acc: 0.8259\n",
            "Epoch 42/300\n",
            "1/1 - 1s - loss: 0.6296 - acc: 0.7863 - val_loss: 0.5078 - val_acc: 0.8263\n",
            "Epoch 43/300\n",
            "1/1 - 1s - loss: 0.6182 - acc: 0.7884 - val_loss: 0.5065 - val_acc: 0.8259\n",
            "Epoch 44/300\n",
            "1/1 - 1s - loss: 0.6230 - acc: 0.7898 - val_loss: 0.5053 - val_acc: 0.8255\n",
            "Epoch 45/300\n",
            "1/1 - 1s - loss: 0.6132 - acc: 0.7911 - val_loss: 0.5042 - val_acc: 0.8255\n",
            "Epoch 46/300\n",
            "1/1 - 1s - loss: 0.6207 - acc: 0.7883 - val_loss: 0.5034 - val_acc: 0.8271\n",
            "Epoch 47/300\n",
            "1/1 - 1s - loss: 0.6090 - acc: 0.7929 - val_loss: 0.5029 - val_acc: 0.8282\n",
            "Epoch 48/300\n",
            "1/1 - 1s - loss: 0.5876 - acc: 0.7999 - val_loss: 0.5023 - val_acc: 0.8282\n",
            "Epoch 49/300\n",
            "1/1 - 1s - loss: 0.6118 - acc: 0.7941 - val_loss: 0.5019 - val_acc: 0.8294\n",
            "Epoch 50/300\n",
            "1/1 - 1s - loss: 0.5824 - acc: 0.8000 - val_loss: 0.5012 - val_acc: 0.8294\n",
            "Epoch 51/300\n",
            "1/1 - 1s - loss: 0.5815 - acc: 0.8027 - val_loss: 0.5004 - val_acc: 0.8302\n",
            "Epoch 52/300\n",
            "1/1 - 1s - loss: 0.5899 - acc: 0.8022 - val_loss: 0.4995 - val_acc: 0.8306\n",
            "Epoch 53/300\n",
            "1/1 - 1s - loss: 0.5874 - acc: 0.8018 - val_loss: 0.4987 - val_acc: 0.8306\n",
            "Epoch 54/300\n",
            "1/1 - 1s - loss: 0.5683 - acc: 0.8076 - val_loss: 0.4980 - val_acc: 0.8298\n",
            "Epoch 55/300\n",
            "1/1 - 1s - loss: 0.5712 - acc: 0.8046 - val_loss: 0.4971 - val_acc: 0.8314\n",
            "Epoch 56/300\n",
            "1/1 - 1s - loss: 0.5684 - acc: 0.8082 - val_loss: 0.4965 - val_acc: 0.8302\n",
            "Epoch 57/300\n",
            "1/1 - 1s - loss: 0.5618 - acc: 0.8073 - val_loss: 0.4958 - val_acc: 0.8306\n",
            "Epoch 58/300\n",
            "1/1 - 1s - loss: 0.5668 - acc: 0.8074 - val_loss: 0.4951 - val_acc: 0.8302\n",
            "Epoch 59/300\n",
            "1/1 - 1s - loss: 0.5544 - acc: 0.8143 - val_loss: 0.4944 - val_acc: 0.8298\n",
            "Epoch 60/300\n",
            "1/1 - 1s - loss: 0.5653 - acc: 0.8069 - val_loss: 0.4937 - val_acc: 0.8310\n",
            "Epoch 61/300\n",
            "1/1 - 1s - loss: 0.5527 - acc: 0.8093 - val_loss: 0.4929 - val_acc: 0.8314\n",
            "Epoch 62/300\n",
            "1/1 - 1s - loss: 0.5628 - acc: 0.8094 - val_loss: 0.4921 - val_acc: 0.8322\n",
            "Epoch 63/300\n",
            "1/1 - 1s - loss: 0.5491 - acc: 0.8150 - val_loss: 0.4913 - val_acc: 0.8310\n",
            "Epoch 64/300\n",
            "1/1 - 1s - loss: 0.5518 - acc: 0.8100 - val_loss: 0.4905 - val_acc: 0.8314\n",
            "Epoch 65/300\n",
            "1/1 - 1s - loss: 0.5424 - acc: 0.8171 - val_loss: 0.4899 - val_acc: 0.8310\n",
            "Epoch 66/300\n",
            "1/1 - 1s - loss: 0.5387 - acc: 0.8128 - val_loss: 0.4892 - val_acc: 0.8314\n",
            "Epoch 67/300\n",
            "1/1 - 1s - loss: 0.5524 - acc: 0.8146 - val_loss: 0.4883 - val_acc: 0.8322\n",
            "Epoch 68/300\n",
            "1/1 - 1s - loss: 0.5397 - acc: 0.8170 - val_loss: 0.4875 - val_acc: 0.8329\n",
            "Epoch 69/300\n",
            "1/1 - 1s - loss: 0.5466 - acc: 0.8122 - val_loss: 0.4868 - val_acc: 0.8318\n",
            "Epoch 70/300\n",
            "1/1 - 1s - loss: 0.5394 - acc: 0.8186 - val_loss: 0.4861 - val_acc: 0.8314\n",
            "Epoch 71/300\n",
            "1/1 - 1s - loss: 0.5320 - acc: 0.8162 - val_loss: 0.4854 - val_acc: 0.8310\n",
            "Epoch 72/300\n",
            "1/1 - 1s - loss: 0.5406 - acc: 0.8099 - val_loss: 0.4848 - val_acc: 0.8314\n",
            "Epoch 73/300\n",
            "1/1 - 1s - loss: 0.5225 - acc: 0.8211 - val_loss: 0.4842 - val_acc: 0.8326\n",
            "Epoch 74/300\n",
            "1/1 - 1s - loss: 0.5288 - acc: 0.8191 - val_loss: 0.4836 - val_acc: 0.8326\n",
            "Epoch 75/300\n",
            "1/1 - 1s - loss: 0.5204 - acc: 0.8219 - val_loss: 0.4831 - val_acc: 0.8326\n",
            "Epoch 76/300\n",
            "1/1 - 1s - loss: 0.5193 - acc: 0.8182 - val_loss: 0.4828 - val_acc: 0.8333\n",
            "Epoch 77/300\n",
            "1/1 - 1s - loss: 0.5202 - acc: 0.8222 - val_loss: 0.4826 - val_acc: 0.8333\n",
            "Epoch 78/300\n",
            "1/1 - 1s - loss: 0.5197 - acc: 0.8204 - val_loss: 0.4825 - val_acc: 0.8333\n",
            "Epoch 79/300\n",
            "1/1 - 1s - loss: 0.5130 - acc: 0.8241 - val_loss: 0.4823 - val_acc: 0.8337\n",
            "Epoch 80/300\n",
            "1/1 - 1s - loss: 0.5175 - acc: 0.8221 - val_loss: 0.4822 - val_acc: 0.8345\n",
            "Epoch 81/300\n",
            "1/1 - 1s - loss: 0.5220 - acc: 0.8227 - val_loss: 0.4821 - val_acc: 0.8337\n",
            "Epoch 82/300\n",
            "1/1 - 1s - loss: 0.5217 - acc: 0.8214 - val_loss: 0.4818 - val_acc: 0.8333\n",
            "Epoch 83/300\n",
            "1/1 - 1s - loss: 0.5069 - acc: 0.8253 - val_loss: 0.4814 - val_acc: 0.8333\n",
            "Epoch 84/300\n",
            "1/1 - 1s - loss: 0.5067 - acc: 0.8260 - val_loss: 0.4811 - val_acc: 0.8329\n",
            "Epoch 85/300\n",
            "1/1 - 1s - loss: 0.5078 - acc: 0.8244 - val_loss: 0.4808 - val_acc: 0.8333\n",
            "Epoch 86/300\n",
            "1/1 - 1s - loss: 0.4999 - acc: 0.8279 - val_loss: 0.4805 - val_acc: 0.8345\n",
            "Epoch 87/300\n",
            "1/1 - 1s - loss: 0.5060 - acc: 0.8251 - val_loss: 0.4800 - val_acc: 0.8357\n",
            "Epoch 88/300\n",
            "1/1 - 1s - loss: 0.5053 - acc: 0.8229 - val_loss: 0.4797 - val_acc: 0.8357\n",
            "Epoch 89/300\n",
            "1/1 - 1s - loss: 0.5079 - acc: 0.8222 - val_loss: 0.4794 - val_acc: 0.8361\n",
            "Epoch 90/300\n",
            "1/1 - 1s - loss: 0.4969 - acc: 0.8270 - val_loss: 0.4793 - val_acc: 0.8369\n",
            "Epoch 91/300\n",
            "1/1 - 1s - loss: 0.4967 - acc: 0.8300 - val_loss: 0.4791 - val_acc: 0.8365\n",
            "Epoch 92/300\n",
            "1/1 - 1s - loss: 0.4971 - acc: 0.8267 - val_loss: 0.4791 - val_acc: 0.8361\n",
            "Epoch 93/300\n",
            "1/1 - 1s - loss: 0.4925 - acc: 0.8287 - val_loss: 0.4791 - val_acc: 0.8361\n",
            "Epoch 94/300\n",
            "1/1 - 1s - loss: 0.5113 - acc: 0.8263 - val_loss: 0.4790 - val_acc: 0.8357\n",
            "Epoch 95/300\n",
            "1/1 - 1s - loss: 0.4913 - acc: 0.8315 - val_loss: 0.4789 - val_acc: 0.8357\n",
            "Epoch 96/300\n",
            "1/1 - 1s - loss: 0.4864 - acc: 0.8315 - val_loss: 0.4786 - val_acc: 0.8369\n",
            "Epoch 97/300\n",
            "1/1 - 1s - loss: 0.4863 - acc: 0.8318 - val_loss: 0.4783 - val_acc: 0.8380\n",
            "Epoch 98/300\n",
            "1/1 - 1s - loss: 0.4976 - acc: 0.8275 - val_loss: 0.4782 - val_acc: 0.8365\n",
            "Epoch 99/300\n",
            "1/1 - 1s - loss: 0.4897 - acc: 0.8320 - val_loss: 0.4780 - val_acc: 0.8369\n",
            "Epoch 100/300\n",
            "1/1 - 1s - loss: 0.4863 - acc: 0.8306 - val_loss: 0.4781 - val_acc: 0.8365\n",
            "Epoch 101/300\n",
            "1/1 - 1s - loss: 0.4816 - acc: 0.8328 - val_loss: 0.4783 - val_acc: 0.8357\n",
            "Epoch 102/300\n",
            "1/1 - 1s - loss: 0.4745 - acc: 0.8381 - val_loss: 0.4787 - val_acc: 0.8349\n",
            "Epoch 103/300\n",
            "1/1 - 1s - loss: 0.4841 - acc: 0.8311 - val_loss: 0.4791 - val_acc: 0.8357\n",
            "Epoch 104/300\n",
            "1/1 - 1s - loss: 0.4823 - acc: 0.8315 - val_loss: 0.4794 - val_acc: 0.8357\n",
            "Epoch 105/300\n",
            "1/1 - 1s - loss: 0.4811 - acc: 0.8328 - val_loss: 0.4795 - val_acc: 0.8349\n",
            "Epoch 106/300\n",
            "1/1 - 1s - loss: 0.4828 - acc: 0.8343 - val_loss: 0.4797 - val_acc: 0.8349\n",
            "Epoch 107/300\n",
            "1/1 - 1s - loss: 0.4813 - acc: 0.8319 - val_loss: 0.4795 - val_acc: 0.8353\n",
            "Epoch 108/300\n",
            "1/1 - 1s - loss: 0.4699 - acc: 0.8346 - val_loss: 0.4792 - val_acc: 0.8357\n",
            "Epoch 109/300\n",
            "1/1 - 1s - loss: 0.4793 - acc: 0.8345 - val_loss: 0.4789 - val_acc: 0.8357\n",
            "Epoch 110/300\n",
            "1/1 - 1s - loss: 0.4712 - acc: 0.8374 - val_loss: 0.4786 - val_acc: 0.8369\n",
            "Epoch 111/300\n",
            "1/1 - 1s - loss: 0.4764 - acc: 0.8344 - val_loss: 0.4784 - val_acc: 0.8369\n",
            "Epoch 112/300\n",
            "1/1 - 1s - loss: 0.4672 - acc: 0.8346 - val_loss: 0.4783 - val_acc: 0.8361\n",
            "Epoch 113/300\n",
            "1/1 - 1s - loss: 0.4762 - acc: 0.8315 - val_loss: 0.4783 - val_acc: 0.8357\n",
            "Epoch 114/300\n",
            "1/1 - 1s - loss: 0.4646 - acc: 0.8396 - val_loss: 0.4784 - val_acc: 0.8361\n",
            "Epoch 115/300\n",
            "1/1 - 1s - loss: 0.4762 - acc: 0.8357 - val_loss: 0.4786 - val_acc: 0.8372\n",
            "Epoch 116/300\n",
            "1/1 - 1s - loss: 0.4746 - acc: 0.8332 - val_loss: 0.4787 - val_acc: 0.8369\n",
            "Epoch 117/300\n",
            "1/1 - 1s - loss: 0.4739 - acc: 0.8316 - val_loss: 0.4788 - val_acc: 0.8369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNgS2hzk3Qzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d749f06c-70fa-4ef4-956e-25c26ddbff52"
      },
      "source": [
        "# We evaluate our best model on our test set\n",
        "test_metrics = model.evaluate(test_gen)\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "    print(\"\\t{}: {:0.4f}\".format(name, val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4783 - acc: 0.8380\n",
            "\n",
            "Test Set Metrics:\n",
            "\tloss: 0.4783\n",
            "\tacc: 0.8380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8rt0eckSyQP",
        "colab_type": "text"
      },
      "source": [
        "### With our GCN model we get a **83.80%** accuracy on the test set! It's performance our a little bit higher than the GAT model and it is now our best model for prediction of labels of scientific research paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW4l2EEV3P2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We make predictions on a new dataset\n",
        "all_nodes = data_dic[\"test\"].index\n",
        "all_mapper = generator.flow(all_nodes)\n",
        "all_predictions = model.predict(all_mapper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HbVk-Sk3aaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f5bffb2-b577-433f-fef5-d3063ef660a4"
      },
      "source": [
        "# As these data are under the shape of the softmax output layers, we need to inverse_transform function to get them under the shape of original categories.\n",
        "node_predictions = target_encoding.inverse_transform(all_predictions.squeeze())\n",
        "node_predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 2, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioIfiqHS3a-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We download the new predictions under a csv format\n",
        "from google.colab import files\n",
        "\n",
        "submit = pd.DataFrame(data=node_predictions , index = data_dic[\"test\"].index, columns = [\"label\"])\n",
        "submit.to_csv('GCN_5.csv') \n",
        "files.download('GCN_5.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}